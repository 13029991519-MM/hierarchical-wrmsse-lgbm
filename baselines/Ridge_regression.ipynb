{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239f23db-960a-4f4d-aa5d-822dc43d5494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage from 0.09 MB drop to 0.10 MB\n",
      "Memory usage from 14.89 MB drop to 4.66 MB\n",
      "Total submission samples: 60980, extracted 3049 unique items, 10 unique stores\n",
      "Memory usage from 208.77 MB drop to 58.80 MB\n",
      "Total sell price records: 6841121, covering 3049 unique items, 10 unique stores\n",
      "========== Start processing validation chunks ==========\n",
      "Memory usage from 146.41 MB drop to 21.00 MB\n",
      "\n",
      "===== Processing chunk 1 (validation=True) =====\n",
      "Memory usage from 420.02 MB drop to 182.93 MB\n",
      "Chunk reshaped to long format, total rows: 19130000\n",
      "Memory usage from 930.85 MB drop to 693.76 MB\n",
      "Training set rows in chunk: 19130000\n",
      "Chunk contains 10000 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 12196\n",
      "Memory usage from 19.54 MB drop to 11.51 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 12196 prediction results\n",
      "Memory usage from 146.41 MB drop to 19.15 MB\n",
      "\n",
      "===== Processing chunk 2 (validation=True) =====\n",
      "Memory usage from 420.02 MB drop to 182.93 MB\n",
      "Chunk reshaped to long format, total rows: 19130000\n",
      "Memory usage from 930.85 MB drop to 693.76 MB\n",
      "Training set rows in chunk: 19130000\n",
      "Chunk contains 10000 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 12196\n",
      "Memory usage from 19.54 MB drop to 11.51 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 12196 prediction results\n",
      "Memory usage from 146.41 MB drop to 19.04 MB\n",
      "\n",
      "===== Processing chunk 3 (validation=True) =====\n",
      "Memory usage from 420.02 MB drop to 182.93 MB\n",
      "Chunk reshaped to long format, total rows: 19130000\n",
      "Memory usage from 930.85 MB drop to 693.76 MB\n",
      "Training set rows in chunk: 19130000\n",
      "Chunk contains 10000 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 12196\n",
      "Memory usage from 19.54 MB drop to 11.51 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 12196 prediction results\n",
      "Memory usage from 7.17 MB drop to 0.94 MB\n",
      "\n",
      "===== Processing chunk 4 (validation=True) =====\n",
      "Memory usage from 20.60 MB drop to 9.06 MB\n",
      "Chunk reshaped to long format, total rows: 937370\n",
      "Memory usage from 50.97 MB drop to 34.09 MB\n",
      "Training set rows in chunk: 937370\n",
      "Chunk contains 490 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 490\n",
      "Memory usage from 0.79 MB drop to 0.49 MB\n",
      "Chunk generated 490 prediction results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Start processing evaluation chunks ==========\n",
      "Memory usage from 148.54 MB drop to 21.26 MB\n",
      "\n",
      "===== Processing chunk 1 (validation=False) =====\n",
      "Memory usage from 426.16 MB drop to 185.60 MB\n",
      "Chunk reshaped to long format, total rows: 19410000\n",
      "Memory usage from 944.47 MB drop to 703.90 MB\n",
      "Training set rows in chunk: 19130000\n",
      "Chunk contains 10000 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 12196\n",
      "Memory usage from 19.54 MB drop to 11.51 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 12196 prediction results\n",
      "Memory usage from 148.54 MB drop to 19.41 MB\n",
      "\n",
      "===== Processing chunk 2 (validation=False) =====\n",
      "Memory usage from 426.16 MB drop to 185.60 MB\n",
      "Chunk reshaped to long format, total rows: 19410000\n",
      "Memory usage from 944.47 MB drop to 703.90 MB\n",
      "Training set rows in chunk: 19130000\n",
      "Chunk contains 10000 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 12196\n",
      "Memory usage from 19.54 MB drop to 11.51 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 12196 prediction results\n",
      "Memory usage from 148.54 MB drop to 19.31 MB\n",
      "\n",
      "===== Processing chunk 3 (validation=False) =====\n",
      "Memory usage from 426.16 MB drop to 185.60 MB\n",
      "Chunk reshaped to long format, total rows: 19410000\n",
      "Memory usage from 944.47 MB drop to 703.90 MB\n",
      "Training set rows in chunk: 19130000\n",
      "Chunk contains 10000 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 12196\n",
      "Memory usage from 19.54 MB drop to 11.51 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 12196 prediction results\n",
      "Memory usage from 7.28 MB drop to 0.95 MB\n",
      "\n",
      "===== Processing chunk 4 (validation=False) =====\n",
      "Memory usage from 20.90 MB drop to 9.19 MB\n",
      "Chunk reshaped to long format, total rows: 951090\n",
      "Memory usage from 51.72 MB drop to 34.58 MB\n",
      "Training set rows in chunk: 937370\n",
      "Chunk contains 490 unique item-store combinations\n",
      "Number of matched test IDs in chunk: 490\n",
      "Memory usage from 0.79 MB drop to 0.49 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:188: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
      "C:\\Users\\Tho'r\\AppData\\Local\\Temp\\ipykernel_19468\\121281277.py:190: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  chunk_sub = test_data.pivot_table(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk generated 490 prediction results\n",
      "\n",
      "========== Merge results ==========\n",
      "Number of valid prediction chunks: 8\n",
      "Remaining unique IDs after deduplication: 60980\n",
      "Final submission file saved to: D:/m5-forecasting-accuracy/M5_ridge_submission.csv\n",
      "Total rows in submission file: 60980\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# ===================== 1. Basic configuration + memory optimization function =====================\n",
    "DATA_DIR = 'D:/m5-forecasting-accuracy/'\n",
    "\n",
    "\n",
    "def reduce_mem_usage(df):\n",
    "    start_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(col_type):\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.api.types.is_integer_dtype(col_type):\n",
    "                if c_min >= 0 and c_max <= 255:\n",
    "                    df[col] = df[col].astype(np.uint8)\n",
    "                elif c_max <= 65535:\n",
    "                    df[col] = df[col].astype(np.uint16)\n",
    "                elif c_max <= 4294967295:\n",
    "                    df[col] = df[col].astype(np.uint32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            elif pd.api.types.is_float_dtype(col_type):\n",
    "                df[col] = df[col].astype(np.float32)\n",
    "        elif col_type == object:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024 ** 2\n",
    "    print(f'Memory usage from {start_mem:.2f} MB drop to {end_mem:.2f} MB')\n",
    "    return df\n",
    "\n",
    "FILES = {\n",
    "    'sales_val': 'sales_train_validation.csv',\n",
    "    'sales_eval': 'sales_train_evaluation.csv',\n",
    "    'calendar': 'calendar.csv',\n",
    "    'sell_prices': 'sell_prices.csv',\n",
    "    'sample_sub': 'sample_submission.csv'\n",
    "}\n",
    "for file_name, file_path in FILES.items():\n",
    "    full_path = os.path.join(DATA_DIR, file_path)\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"File does not exist: {full_path}\")\n",
    "\n",
    "# ===================== 2. Read and preprocess the calendar table =====================\n",
    "calendar = pd.read_csv(os.path.join(DATA_DIR, FILES['calendar']), encoding='latin1')\n",
    "calendar['day_number'] = calendar['d'].str.extract(r'd_(\\d+)').astype(int)\n",
    "calendar = calendar[['d', 'day_number', 'wm_yr_wk', 'wday', 'month', 'year']]\n",
    "calendar = reduce_mem_usage(calendar)\n",
    "\n",
    "# ===================== 3. Read submission sample (fix item/store extraction logic) =====================\n",
    "sample_sub = pd.read_csv(os.path.join(DATA_DIR, FILES['sample_sub']), encoding='latin1')\n",
    "# Fix 1: More robust dataset extraction (avoid index out of bounds after splitting)\n",
    "sample_sub['dataset'] = sample_sub['id'].apply(lambda x: x.split('_')[-1] if '_' in x else '')\n",
    "\n",
    "\n",
    "# Fix 2: Item_id/store_id extraction (compatible with all ID formats)\n",
    "def extract_item_store(id_str):\n",
    "    # Example ID: HOBBIES_1_001_CA_1_validation â†’ remove the last field after splitting\n",
    "    parts = id_str.split('_')\n",
    "    if len(parts) >= 4:\n",
    "        # Take the first n-3 parts to form item_id, last two parts to form store_id\n",
    "        item_id = '_'.join(parts[:-3])\n",
    "        store_id = '_'.join(parts[-3:-1])\n",
    "    else:\n",
    "        item_id = ''\n",
    "        store_id = ''\n",
    "    return item_id, store_id\n",
    "\n",
    "\n",
    "# Batch extraction\n",
    "sample_sub[['item_id', 'store_id']] = sample_sub['id'].apply(\n",
    "    lambda x: pd.Series(extract_item_store(x))\n",
    ")\n",
    "sample_sub = reduce_mem_usage(sample_sub)\n",
    "print(\n",
    "    f\"Total submission samples: {len(sample_sub)}, extracted {sample_sub['item_id'].nunique()} unique items, {sample_sub['store_id'].nunique()} unique stores\")\n",
    "\n",
    "# ===================== 4. Read sell prices table =====================\n",
    "sell_prices = pd.read_csv(os.path.join(DATA_DIR, FILES['sell_prices']), encoding='latin1')\n",
    "sell_prices = reduce_mem_usage(sell_prices)\n",
    "print(\n",
    "    f\"Total sell price records: {len(sell_prices)}, covering {sell_prices['item_id'].nunique()} unique items, {sell_prices['store_id'].nunique()} unique stores\")\n",
    "\n",
    "\n",
    "# ===================== 5. Chunk processing function (with detailed logs) =====================\n",
    "def process_sales_chunk(chunk, is_validation, chunk_idx):\n",
    "    print(f\"\\n===== Processing chunk {chunk_idx} (validation={is_validation}) =====\")\n",
    "    # Reshape wide sales table to long format\n",
    "    d_cols = [col for col in chunk.columns if col.startswith('d_')]\n",
    "    if not d_cols:\n",
    "        print(\"No date columns in chunk, skipped\")\n",
    "        return None\n",
    "    df_melt = chunk.melt(\n",
    "        id_vars=['id', 'item_id', 'store_id'],\n",
    "        value_vars=d_cols,\n",
    "        var_name='d',\n",
    "        value_name='sales'\n",
    "    )\n",
    "    df_melt['dataset'] = 'validation' if is_validation else 'evaluation'\n",
    "    df_melt = reduce_mem_usage(df_melt)\n",
    "    print(f\"Chunk reshaped to long format, total rows: {len(df_melt)}\")\n",
    "\n",
    "    # Merge calendar features\n",
    "    df_melt = df_melt.merge(calendar, on='d', how='left')\n",
    "    # Generate cyclic features\n",
    "    df_melt['wday_sin'] = np.sin(2 * math.pi * df_melt['wday'] / 7).astype(np.float32)\n",
    "    df_melt['wday_cos'] = np.cos(2 * math.pi * df_melt['wday'] / 7).astype(np.float32)\n",
    "    df_melt['month_sin'] = np.sin(2 * math.pi * df_melt['month'] / 12).astype(np.float32)\n",
    "    df_melt['month_cos'] = np.cos(2 * math.pi * df_melt['month'] / 12).astype(np.float32)\n",
    "\n",
    "    # Merge sell price features\n",
    "    df_melt = df_melt.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    df_melt['sell_price'] = df_melt['sell_price'].fillna(0).astype(np.float32)\n",
    "    df_melt = reduce_mem_usage(df_melt)\n",
    "\n",
    "    # Split training set\n",
    "    train_data = df_melt[df_melt['day_number'] <= 1913].copy()\n",
    "    print(f\"Training set rows in chunk: {len(train_data)}\")\n",
    "    if len(train_data) == 0:\n",
    "        print(\"Training set is empty, skipped\")\n",
    "        return None\n",
    "\n",
    "    # Construct test set\n",
    "    chunk_item_stores = df_melt[['item_id', 'store_id']].drop_duplicates()\n",
    "    print(f\"Chunk contains {len(chunk_item_stores)} unique item-store combinations\")\n",
    "\n",
    "    test_ids = sample_sub[\n",
    "        (sample_sub['item_id'].isin(chunk_item_stores['item_id'])) &\n",
    "        (sample_sub['store_id'].isin(chunk_item_stores['store_id'])) &\n",
    "        (sample_sub['dataset'] == ('validation' if is_validation else 'evaluation'))  # New: Match dataset type\n",
    "        ]\n",
    "    print(f\"Number of matched test IDs in chunk: {len(test_ids)}\")\n",
    "    if len(test_ids) == 0:\n",
    "        print(\"No matched test IDs, skipped\")\n",
    "        return None\n",
    "\n",
    "    # Generate test set date rows\n",
    "    pred_days = {\n",
    "        'validation': [f'd_{i}' for i in range(1914, 1942)],\n",
    "        'evaluation': [f'd_{i}' for i in range(1942, 1970)]\n",
    "    }\n",
    "    test_data_list = []\n",
    "    for _, row in test_ids.iterrows():\n",
    "        for d in pred_days[row['dataset']]:\n",
    "            test_data_list.append({\n",
    "                'id': row['id'],\n",
    "                'item_id': row['item_id'],\n",
    "                'store_id': row['store_id'],\n",
    "                'd': d\n",
    "            })\n",
    "    test_data = pd.DataFrame(test_data_list)\n",
    "    test_data = test_data.merge(calendar, on='d', how='left')\n",
    "    # Generate cyclic features for test set\n",
    "    test_data['wday_sin'] = np.sin(2 * math.pi * test_data['wday'] / 7).astype(np.float32)\n",
    "    test_data['wday_cos'] = np.cos(2 * math.pi * test_data['wday'] / 7).astype(np.float32)\n",
    "    test_data['month_sin'] = np.sin(2 * math.pi * test_data['month'] / 12).astype(np.float32)\n",
    "    test_data['month_cos'] = np.cos(2 * math.pi * test_data['month'] / 12).astype(np.float32)\n",
    "    # Merge sell price features\n",
    "    test_data = test_data.merge(sell_prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "    test_data['sell_price'] = test_data['sell_price'].fillna(0).astype(np.float32)\n",
    "    test_data = reduce_mem_usage(test_data)\n",
    "\n",
    "    # Build model\n",
    "    X_train = train_data[['wday_sin', 'wday_cos', 'month_sin', 'month_cos', 'year', 'sell_price']]\n",
    "    y_train = train_data['sales']\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', StandardScaler(), ['wday_sin', 'wday_cos', 'month_sin', 'month_cos', 'year', 'sell_price'])]\n",
    "    )\n",
    "    model = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', Ridge(alpha=1.0))])\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    X_test = test_data[['wday_sin', 'wday_cos', 'month_sin', 'month_cos', 'year', 'sell_price']]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.maximum(y_pred, 0).astype(np.float32)\n",
    "    test_data['sales_pred'] = y_pred\n",
    "\n",
    "    # Organize results\n",
    "    test_data['F'] = test_data.groupby('id')['day_number'].rank(method='first').astype(int)\n",
    "    test_data['F_col'] = 'F' + test_data['F'].astype(str)\n",
    "    chunk_sub = test_data.pivot_table(\n",
    "        index='id', columns='F_col', values='sales_pred', aggfunc='first'\n",
    "    ).reset_index()\n",
    "    chunk_sub = chunk_sub[['id'] + [f'F{i}' for i in range(1, 29)]]\n",
    "    print(f\"Chunk generated {len(chunk_sub)} prediction results\")\n",
    "    return chunk_sub\n",
    "\n",
    "\n",
    "# ===================== 6. Execute chunk processing =====================\n",
    "chunk_size = 10000\n",
    "submission_list = []\n",
    "\n",
    "# Process validation sales data\n",
    "print(\"========== Start processing validation chunks ==========\")\n",
    "for i, chunk in enumerate(\n",
    "        pd.read_csv(os.path.join(DATA_DIR, FILES['sales_val']), chunksize=chunk_size, encoding='latin1')):\n",
    "    chunk = reduce_mem_usage(chunk)\n",
    "    chunk_sub = process_sales_chunk(chunk, is_validation=True, chunk_idx=i + 1)\n",
    "    if chunk_sub is not None:\n",
    "        submission_list.append(chunk_sub)\n",
    "\n",
    "# Process evaluation sales data (Optional: Comment first to validate validation only)\n",
    "print(\"\\n========== Start processing evaluation chunks ==========\")\n",
    "for i, chunk in enumerate(pd.read_csv(os.path.join(DATA_DIR, FILES['sales_eval']), chunksize=chunk_size, encoding='latin1')):\n",
    "\n",
    "    chunk = reduce_mem_usage(chunk)\n",
    "    chunk_sub = process_sales_chunk(chunk, is_validation=False, chunk_idx=i+1)\n",
    "    if chunk_sub is not None:\n",
    "         submission_list.append(chunk_sub)\n",
    "\n",
    "# ===================== 7. Merge results (add deduplication logic) =====================\n",
    "print(f\"\\n========== Merge results ==========\")\n",
    "print(f\"Number of valid prediction chunks: {len(submission_list)}\")\n",
    "\n",
    "# Fallback: Generate all-zero submission if no results\n",
    "if not submission_list:\n",
    "    print(\"No valid prediction results, generate all-zero submission file\")\n",
    "    submission = sample_sub[['id']].copy()\n",
    "    for f in [f'F{i}' for i in range(1, 29)]:\n",
    "        submission[f] = 0.0\n",
    "else:\n",
    "    submission = pd.concat(submission_list, ignore_index=True)\n",
    "    # Key modification: Deduplicate by ID, keep the last row for each ID\n",
    "    submission = submission.drop_duplicates(subset='id', keep='last').reset_index(drop=True)\n",
    "    print(f\"Remaining unique IDs after deduplication: {len(submission)}\")\n",
    "\n",
    "    # Supplement missing IDs\n",
    "    all_ids = sample_sub['id'].tolist()\n",
    "    missing_ids = [id_ for id_ in all_ids if id_ not in submission['id'].tolist()]\n",
    "    if missing_ids:\n",
    "        missing_df = pd.DataFrame({'id': missing_ids})\n",
    "        for f in [f'F{i}' for i in range(1, 29)]:\n",
    "            missing_df[f] = 0.0\n",
    "        submission = pd.concat([submission, missing_df], ignore_index=True)\n",
    "\n",
    "    # Sort by original ID order\n",
    "    submission = submission.set_index('id').loc[all_ids].reset_index()\n",
    "\n",
    "# Save submission file\n",
    "submission_path = os.path.join(DATA_DIR, 'M5_ridge_submission.csv')\n",
    "submission.to_csv(submission_path, index=False, encoding='latin1')\n",
    "print(f\"Final submission file saved to: {submission_path}\")\n",
    "print(f\"Total rows in submission file: {len(submission)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d0d545-e891-48a2-9f97-97bed709b488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
