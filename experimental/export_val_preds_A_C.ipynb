{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Export validation predictions (d_1914\u9225\u63f9_1941) for two LGBM variants:<br>\n", "- A: baseline feature set (full features), uses STORE_PARAMS if available else BASE_PARAMS.<br>\n", "- C: mask price/discount features (same as train_lgbm_mask_c.py).<br>\n", "Outputs:<br>\n", "- weight_v2/preds_val_A.csv<br>\n", "- weight_v2/preds_val_C.csv<br>\n", "Default stores: CA_1, TX_1, WI_1 (configurable via --stores).<br>\n", "Does not overwrite submissions; for offline weight search only.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import annotations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import json\n", "from pathlib import Path\n", "from typing import List, Tuple"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from train_lgbm_baseline import (\n", "    BIN_COLS,\n", "    CAT_COLS,\n", "    CYCLIC,\n", "    NUM_SCALED,\n", "    STORE_PARAMS,\n", "    TARGET_COL,\n", "    TRAIN_END,\n", "    VAL_END,\n", "    safe_mape,\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Paths"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATA_DIR = Path(\"newfinaldata\")\n", "OUT_DIR = Path(\"weight_v2\")\n", "OUT_DIR.mkdir(parents=True, exist_ok=True)\n", "OUT_A = OUT_DIR / \"preds_val_A.csv\"\n", "OUT_C = OUT_DIR / \"preds_val_C.csv\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Mask price/discount cols for variant C"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["PRICE_COLS = [c for c in NUM_SCALED if \"price\" in c or \"discount\" in c or \"promo\" in c]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Params"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BASE_PARAMS = dict(\n", "    objective=\"regression\",\n", "    metric=[\"rmse\", \"mape\"],\n", "    learning_rate=0.05,\n", "    num_leaves=255,\n", "    max_depth=10,\n", "    feature_fraction=0.85,\n", "    bagging_fraction=0.85,\n", "    bagging_freq=5,\n", "    min_data_in_leaf=200,\n", "    lambda_l1=0.5,\n", "    lambda_l2=1.0,\n", "    n_estimators=2400,\n", "    max_bin=511,\n", ")\n", "PARAMS_C = dict(\n", "    objective=\"regression\",\n", "    metric=[\"rmse\", \"mape\"],\n", "    learning_rate=0.05,\n", "    num_leaves=255,\n", "    max_depth=10,\n", "    feature_fraction=0.9,\n", "    bagging_fraction=0.9,\n", "    bagging_freq=5,\n", "    min_data_in_leaf=240,\n", "    lambda_l1=0.5,\n", "    lambda_l2=1.0,\n", "    n_estimators=2400,\n", "    max_bin=511,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_args() -> argparse.Namespace:\n", "    p = argparse.ArgumentParser(description=\"Export validation preds for A and C variants.\")\n", "    p.add_argument(\"--stores\", type=str, default=\"CA_1,TX_1,WI_1\", help=\"Comma-separated store_ids to run.\")\n", "    return p.parse_args()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_store(store: str, usecols: List[str]) -> pd.DataFrame:\n", "    path = DATA_DIR / f\"processed_{store}.csv\"\n", "    if not path.exists():\n", "        raise FileNotFoundError(path)\n", "    df = pd.read_csv(path, usecols=usecols)\n", "    # cast categories/bools later\n", "    df[\"d_int\"] = df[\"d\"].str.replace(\"d_\", \"\", regex=False).astype(int)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_datasets(stores: List[str], feature_cols: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n", "    usecols = feature_cols + [TARGET_COL, \"d\"]\n", "    dfs = [read_store(s, usecols) for s in stores]\n", "    df = pd.concat(dfs, ignore_index=True)\n", "    train_df = df[df[\"d_int\"] <= TRAIN_END].copy()\n", "    val_df = df[(df[\"d_int\"] > TRAIN_END) & (df[\"d_int\"] <= VAL_END)].copy()\n", "    for col in CAT_COLS:\n", "        if col in train_df:\n", "            cats = pd.CategoricalDtype(categories=train_df[col].dropna().unique())\n", "            train_df[col] = train_df[col].astype(cats)\n", "            val_df[col] = val_df[col].astype(cats)\n", "    for col in BIN_COLS:\n", "        if col in train_df:\n", "            train_df[col] = train_df[col].astype(\"int8\")\n", "            val_df[col] = val_df[col].astype(\"int8\")\n", "    for col in feature_cols:\n", "        if col not in CAT_COLS + BIN_COLS and col in train_df:\n", "            train_df[col] = train_df[col].astype(\"float32\")\n", "            val_df[col] = val_df[col].astype(\"float32\")\n", "    return train_df, val_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_and_pred(train_df: pd.DataFrame, val_df: pd.DataFrame, feature_cols: List[str], params: dict) -> Tuple[pd.DataFrame, dict]:\n", "    X_train = train_df[feature_cols]\n", "    y_train = train_df[TARGET_COL].astype(\"float32\")\n", "    X_val = val_df[feature_cols]\n", "    y_val = val_df[TARGET_COL].astype(\"float32\")\n", "    model = lgb.LGBMRegressor(**params)\n", "    model.fit(\n", "        X_train,\n", "        y_train,\n", "        eval_set=[(X_val, y_val)],\n", "        eval_metric=\"rmse\",\n", "        categorical_feature=CAT_COLS,\n", "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n", "    )\n", "    preds = model.predict(X_val, num_iteration=model.best_iteration_)\n", "    rmse = float(np.sqrt(mean_squared_error(y_val, preds)))\n", "    mape = safe_mape(y_val.values, preds)\n", "    return pd.DataFrame({\"id\": val_df[\"id\"], \"pred\": preds, \"d_int\": val_df[\"d_int\"]}), {\"rmse\": rmse, \"mape\": mape, \"iter\": model.best_iteration_}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def pivot_preds(df: pd.DataFrame) -> pd.DataFrame:\n", "    \"\"\"Pivot to wide F1..F28; keeps only d_1914..d_1941 to avoid huge matrices.\"\"\"\n", "    df = df.copy()\n", "    df = df[(df[\"d_int\"] >= 1914) & (df[\"d_int\"] <= 1941)]\n", "    pivot = df.pivot(index=\"id\", columns=\"d_int\", values=\"pred\").astype(\"float32\")\n", "    cols = list(range(1914, 1942))\n", "    missing = [c for c in cols if c not in pivot.columns]\n", "    if missing:\n", "        raise ValueError(f\"Missing d columns for validation window: {missing}\")\n", "    pivot = pivot[cols]\n", "    pivot.columns = [f\"F{i}\" for i in range(1, 29)]\n", "    pivot.reset_index(inplace=True)\n", "    return pivot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main() -> None:\n", "    args = parse_args()\n", "    stores = [s.strip() for s in args.stores.split(\",\") if s.strip()]\n\n", "    # Model A (full)\n", "    feature_cols_a = NUM_SCALED + CYCLIC + BIN_COLS + CAT_COLS\n", "    train_a, val_a = build_datasets(stores, feature_cols_a)\n", "    params_a = STORE_PARAMS.get(stores[0], BASE_PARAMS) if len(stores) == 1 else BASE_PARAMS\n", "    preds_a, metrics_a = train_and_pred(train_a, val_a, feature_cols_a, params_a)\n", "    pivot_a = pivot_preds(preds_a)\n", "    pivot_a.to_csv(OUT_A, index=False)\n", "    print(f\"Saved A preds to {OUT_A}, metrics {metrics_a}\")\n\n", "    # Model C (mask price)\n", "    feature_cols_c = [c for c in NUM_SCALED if c not in PRICE_COLS] + CYCLIC + BIN_COLS + CAT_COLS\n", "    train_c, val_c = build_datasets(stores, feature_cols_c)\n", "    preds_c, metrics_c = train_and_pred(train_c, val_c, feature_cols_c, PARAMS_C)\n", "    pivot_c = pivot_preds(preds_c)\n", "    pivot_c.to_csv(OUT_C, index=False)\n", "    print(f\"Saved C preds to {OUT_C}, metrics {metrics_c}\")\n", "    summary = {\n", "        \"stores\": stores,\n", "        \"A\": metrics_a,\n", "        \"C\": metrics_c,\n", "    }\n", "    with (OUT_DIR / \"summary_val_A_C.json\").open(\"w\", encoding=\"utf-8\") as f:\n", "        json.dump(summary, f, ensure_ascii=False, indent=2)\n", "    print(f\"Saved summary to {OUT_DIR/'summary_val_A_C.json'}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}