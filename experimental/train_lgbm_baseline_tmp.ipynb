{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3\n", "# -*- coding: utf-8 -*-\n", "\"\"\"\n", "LightGBM baseline training script for M5-style data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Usage:\n", "    python train_lgbm_baseline.py"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["What it does:\n", "- Loads selected store files from `newdata_evaluation/processed_*.csv`.\n", "- Splits by time: train d_1\u20131913, valid d_1914\u20131941.\n", "- Fits categorical encoders on train only, then trains LightGBM.\n", "- Reports RMSE/MAPE on the validation split."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Adjustable knobs:\n", "- STORES: which store files to use (start small, then add more).\n", "- DATA_DIR: use newfinaldata for quick dry runs (only d_1\u20131913, no val).\n", "- LIGHTGBM_PARAMS: tweak leaves/learning_rate, etc.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import annotations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gc\n", "import random\n", "from pathlib import Path\n", "from typing import List"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import mean_squared_error\n", "import json"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from wrmsse_official import WRMSSEEvaluator  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["??/??? WRMSSE evaluation ID ? validation ?????"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WRMSSE_SALES_FILE = Path(\"data/sales_train_validation.csv\")\n", "WRMSSE_ENABLED = False\n", "try:\n", "    _WRMSSE_OFFICIAL = WRMSSEEvaluator(sales_file=WRMSSE_SALES_FILE) if WRMSSE_ENABLED else None\n", "except Exception as e:\n", "    print(f\"Warning: WRMSSEEvaluator import/init failed ({e}); wrmsse will raise if called.\")\n", "    _WRMSSE_OFFICIAL = None"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Force disable WRMSSE during tuning"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WRMSSE_ENABLED = False\n", "_WRMSSE_OFFICIAL = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WEIGHT_DIR = Path(\"weight\")\n", "WEIGHT_DIR.mkdir(exist_ok=True, parents=True)\n", "SUMMARY_PATH = WEIGHT_DIR / \"summary_auto.json\"\n", "BLEND_WEIGHT_PATH = WEIGHT_DIR / \"blend_weights_auto.json\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STATE_GROUPS = {\n", "    \"CA\": [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"],\n", "    \"TX\": [\"TX_1\", \"TX_2\", \"TX_3\"],\n", "    \"WI\": [\"WI_1\", \"WI_2\", \"WI_3\"],\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["LEVEL_MODE = \"store\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["All stores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STORE_LIST: List[str] = [s for stores in STATE_GROUPS.values() for s in stores]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Stores that should force random search even if STORE_PARAMS exists<br>\n", "Empty: use fixed per-store params for all stores."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SEARCH_STORES = set(STORE_LIST) - {\"CA_3\", \"WI_2\"}  # ????"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Store-specific tuned params (will override BASE_PARAMS when present)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STORE_PARAMS = {\n", "    \"CA_1\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.03291170222996989,\n", "        \"num_leaves\": 383,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.7987286816083341,\n", "        \"bagging_fraction\": 0.8443754341594838,\n", "        \"bagging_freq\": 5,\n", "        \"min_data_in_leaf\": 120,\n", "        \"lambda_l1\": 0.1,\n", "        \"lambda_l2\": 0.5,\n", "        \"n_estimators\": 2000,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"CA_2\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.021571100632025438,\n", "        \"num_leaves\": 255,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.8836245222229365,\n", "        \"bagging_fraction\": 0.8328070668543215,\n", "        \"bagging_freq\": 3,\n", "        \"min_data_in_leaf\": 120,\n", "        \"lambda_l1\": 0.5,\n", "        \"lambda_l2\": 1.0,\n", "        \"n_estimators\": 1800,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"CA_3\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.019738923414965245,\n", "        \"num_leaves\": 319,\n", "        \"max_depth\": 10,\n", "        \"feature_fraction\": 0.8714576239732946,\n", "        \"bagging_fraction\": 0.8080588969530361,\n", "        \"bagging_freq\": 3,\n", "        \"min_data_in_leaf\": 150,\n", "        \"lambda_l1\": 0.5,\n", "        \"lambda_l2\": 0.5,\n", "        \"n_estimators\": 2000,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"CA_4\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.02654910862466569,\n", "        \"num_leaves\": 383,\n", "        \"max_depth\": 10,\n", "        \"feature_fraction\": 0.9834467544005815,\n", "        \"bagging_fraction\": 0.7906891653855229,\n", "        \"bagging_freq\": 5,\n", "        \"min_data_in_leaf\": 180,\n", "        \"lambda_l1\": 0.5,\n", "        \"lambda_l2\": 0.5,\n", "        \"n_estimators\": 2200,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"TX_1\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.030801474444336212,\n", "        \"num_leaves\": 319,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.8342369743978673,\n", "        \"bagging_fraction\": 0.9899973308685395,\n", "        \"bagging_freq\": 7,\n", "        \"min_data_in_leaf\": 180,\n", "        \"lambda_l1\": 0.1,\n", "        \"lambda_l2\": 0.5,\n", "        \"n_estimators\": 1800,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"TX_2\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.02620774853906023,\n", "        \"num_leaves\": 383,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.8002060361355822,\n", "        \"bagging_fraction\": 0.8746486273764073,\n", "        \"bagging_freq\": 5,\n", "        \"min_data_in_leaf\": 150,\n", "        \"lambda_l1\": 1.0,\n", "        \"lambda_l2\": 1.0,\n", "        \"n_estimators\": 1800,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"TX_3\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.01861087438340826,\n", "        \"num_leaves\": 255,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.8177510797053789,\n", "        \"bagging_fraction\": 0.7873106339756981,\n", "        \"bagging_freq\": 3,\n", "        \"min_data_in_leaf\": 120,\n", "        \"lambda_l1\": 1.0,\n", "        \"lambda_l2\": 1.0,\n", "        \"n_estimators\": 1800,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"WI_1\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.02814410986626996,\n", "        \"num_leaves\": 383,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.9762849449709258,\n", "        \"bagging_fraction\": 0.7900643911832694,\n", "        \"bagging_freq\": 7,\n", "        \"min_data_in_leaf\": 180,\n", "        \"lambda_l1\": 0.5,\n", "        \"lambda_l2\": 1.0,\n", "        \"n_estimators\": 2200,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"WI_2\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.02797710656495476,\n", "        \"num_leaves\": 319,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.8225640975142336,\n", "        \"bagging_fraction\": 0.7261280295918504,\n", "        \"bagging_freq\": 3,\n", "        \"min_data_in_leaf\": 150,\n", "        \"lambda_l1\": 0.5,\n", "        \"lambda_l2\": 2.0,\n", "        \"n_estimators\": 2000,\n", "        \"max_bin\": 511,\n", "    },\n", "    \"WI_3\": {\n", "        \"objective\": \"regression\",\n", "        \"metric\": [\"rmse\", \"mape\"],\n", "        \"learning_rate\": 0.03125218432826508,\n", "        \"num_leaves\": 383,\n", "        \"max_depth\": -1,\n", "        \"feature_fraction\": 0.8555882063960265,\n", "        \"bagging_fraction\": 0.8892247633789941,\n", "        \"bagging_freq\": 5,\n", "        \"min_data_in_leaf\": 120,\n", "        \"lambda_l1\": 0.5,\n", "        \"lambda_l2\": 1.0,\n", "        \"n_estimators\": 2200,\n", "        \"max_bin\": 511,\n", "    },\n", "}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For dept/item-level training, optionally restrict to a subset (empty list => all)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DEPT_FILTER: List[str] = []  # e.g., [\"FOODS_1\", \"HOUSEHOLD_1\"]\n", "ITEM_FILTER: List[str] = []  # e.g., [\"FOODS_1_001\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Directory containing processed CSVs.<br>\n", "Use \"newdata_evaluation\" for full 1941d history; \"newfinaldata\" only has d_1?913."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATA_DIR = Path(\"newfinaldata\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Time split (matching M5 validation scheme)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["TRAIN_END = 1913  # inclusive\n", "VAL_END = 1941  # inclusive"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Feature lists"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["NUM_SCALED = [\n", "    \"sell_price_scaled\",\n", "    \"baseline_price_scaled\",\n", "    \"discount_scaled\",\n", "    \"promo_intensity_scaled\",\n", "    \"lag_7_scaled\",\n", "    \"lag_30_scaled\",\n", "    \"rolling_mean_7_scaled\",\n", "    \"rolling_mean_30_scaled\",\n", "    \"rolling_std_7_scaled\",\n", "    \"rolling_std_30_scaled\",\n", "    \"lag_1_scaled\",\n", "    \"lag_14_scaled\",\n", "    \"lag_28_scaled\",\n", "    \"lag_56_scaled\",\n", "    \"lag_84_scaled\",\n", "    \"rolling_mean_14_scaled\",\n", "    \"rolling_mean_28_scaled\",\n", "    \"rolling_mean_56_scaled\",\n", "    \"rolling_mean_84_scaled\",\n", "    \"rolling_std_14_scaled\",\n", "    \"rolling_std_28_scaled\",\n", "    \"rolling_std_56_scaled\",\n", "    \"rolling_std_84_scaled\",\n", "    \"price_ratio_scaled\",\n", "    \"discount_pct_scaled\",\n", "    \"snap_wday_scaled\",\n", "    \"promo_holiday_scaled\",\n", "    \"promo_wday_sin_scaled\",\n", "    \"promo_wday_cos_scaled\",\n", "    \"discount_snap_scaled\",\n", "    # rolling median/min/max (scaled)\n", "    \"rolling_median_7_scaled\",\n", "    \"rolling_median_14_scaled\",\n", "    \"rolling_median_28_scaled\",\n", "    \"rolling_median_30_scaled\",\n", "    \"rolling_median_56_scaled\",\n", "    \"rolling_median_84_scaled\",\n", "    \"rolling_min_7_scaled\",\n", "    \"rolling_min_14_scaled\",\n", "    \"rolling_min_28_scaled\",\n", "    \"rolling_min_30_scaled\",\n", "    \"rolling_min_56_scaled\",\n", "    \"rolling_min_84_scaled\",\n", "    \"rolling_max_7_scaled\",\n", "    \"rolling_max_14_scaled\",\n", "    \"rolling_max_28_scaled\",\n", "    \"rolling_max_30_scaled\",\n", "    \"rolling_max_56_scaled\",\n", "    \"rolling_max_84_scaled\",\n", "    # price momentum / z-score (scaled)\n", "    \"sell_price_week_chg_scaled\",\n", "    \"sell_price_month_chg_scaled\",\n", "    \"sell_price_z28_scaled\",\n", "    \"discount_week_chg_scaled\",\n", "    \"discount_month_chg_scaled\",\n", "    \"discount_z28_scaled\",\n", "    # streaks / holiday distance (scaled)\n", "    \"promo_streak_scaled\",\n", "    \"holiday_streak_scaled\",\n", "    \"days_since_holiday_scaled\",\n", "    \"days_until_holiday_scaled\",\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CYCLIC = [\"wday_sin\", \"wday_cos\", \"month_sin\", \"month_cos\", \"quarter_sin\", \"quarter_cos\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BIN_COLS = [\"snap_CA\", \"snap_TX\", \"snap_WI\", \"IsHoliday\", \"IsPromotion\"]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CAT_COLS = [\n", "    \"state_id\",\n", "    \"store_id\",\n", "    \"cat_id\",\n", "    \"dept_id\",\n", "    \"event_name_1\",\n", "    \"event_type_1\",\n", "    \"event_name_2\",\n", "    \"event_type_2\",\n", "    \"item_id\",\n", "    \"id\",\n", "]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["TARGET_COL = \"sales\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["LightGBM hyperparameters (tuned baseline)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BASE_PARAMS = dict(\n", "    objective=\"regression\",\n", "    metric=[\"rmse\", \"mape\"],\n", "    learning_rate=0.03,\n", "    num_leaves=319,\n", "    max_depth=-1,\n", "    feature_fraction=0.8038614760496599,\n", "    bagging_fraction=0.7567022188313345,\n", "    bagging_freq=5,\n", "    min_data_in_leaf=120,\n", "    lambda_l1=0.1,\n", "    lambda_l2=1.0,\n", "    n_estimators=1800,  # allow more trees; early stopping will pick best_iter\n", "    max_bin=511,  # higher binning for high-cardinality categories\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Random search config"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DO_RANDOM_SEARCH = True  # enable_search\n", "N_TRIALS = 10\n", "EARLY_STOPPING = 100  # more patient on full data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Optional simple blending (primary + one or two tweaked variants)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["ENABLE_BLEND = False  # ?\u03bd???\u03ba????\n", "BLEND_WEIGHTS = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]  # allow pure base or alt\n", "NO_BLEND_STORES = {\"TX_1\", \"WI_3\"}  # skip blending for these stores"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Time series CV config (per-store)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["USE_CV = False  # global default\n", "CV_STORES = {\"CA_3\", \"WI_2\"}  # stores to force CV even if USE_CV=False\n", "CV_VAL_LEN = 28\n", "CV_FOLDS = 2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Target transform (log1p helps long-tail); if False uses raw sales"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["USE_LOG1P_TARGET = False"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Time-decay weighting for recent days (applied to training only)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["TIME_DECAY = True  # set True to enable\n", "DECAY_HALF_LIFE = 90  # days; smaller => heavier recent weighting"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Incremental summary log (jsonl) to avoid losing progress if a later task crashes"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["SUMMARY_PATH = Path(\"logs/summary_lgbm.jsonl\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["----------------------<br>\n", "Helpers<br>\n", "----------------------"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_store(store: str, data_dir: Path, usecols: List[str]) -> pd.DataFrame:\n", "    path = data_dir / f\"processed_{store}.csv\"\n", "    if not path.exists():\n", "        raise FileNotFoundError(f\"{path} not found\")\n", "    df = pd.read_csv(path, usecols=usecols, engine=\"pyarrow\")\n", "    df[\"d_int\"] = df[\"d\"].str.replace(\"d_\", \"\", regex=False).astype(int)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def set_categorical(train: pd.Series, val: pd.Series) -> tuple[pd.Series, pd.Series]:\n", "    cats = pd.CategoricalDtype(categories=train.dropna().unique())\n", "    return train.astype(cats), val.astype(cats)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_datasets(\n", "    stores: List[str],\n", "    dept_filter: List[str] | None = None,\n", "    item_filter: List[str] | None = None,\n", ") -> tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n", "    # Columns that exist in CSV\n", "    feature_cols = NUM_SCALED + CYCLIC + BIN_COLS + CAT_COLS\n", "    generated_cols = [\n", "        \"item_freq\",\n", "        \"id_freq\",\n", "        \"item_freq_scaled\",\n", "        \"id_freq_scaled\",\n", "        \"item_te\",\n", "        \"id_te\",\n", "        \"item_te_scaled\",\n", "        \"id_te_scaled\",\n", "    ]\n", "    usecols = [c for c in feature_cols + [TARGET_COL, \"d\"] if c not in generated_cols]\n", "    dfs = [read_store(store, DATA_DIR, usecols) for store in stores]\n", "    df = pd.concat(dfs, ignore_index=True)\n", "    del dfs\n", "    gc.collect()\n\n", "    # Optional filters\n", "    if dept_filter:\n", "        df = df[df[\"dept_id\"].isin(dept_filter)]\n", "    if item_filter:\n", "        df = df[df[\"item_id\"].isin(item_filter)]\n\n", "    # Split\n", "    train_df = df[df[\"d_int\"] <= TRAIN_END].copy()\n", "    val_df = df[(df[\"d_int\"] > TRAIN_END) & (df[\"d_int\"] <= VAL_END)].copy()\n", "    if val_df.empty:\n", "        raise ValueError(\"Validation set is empty. Use newdata_evaluation or adjust VAL_END.\")\n\n", "    # Categorical encoding: fit on train, transform val\n", "    for col in CAT_COLS:\n", "        train_df[col], val_df[col] = set_categorical(train_df[col].astype(str), val_df[col].astype(str))\n\n", "    # Frequency encoding for high-cardinality item_id/id (fit on train only)\n", "    item_counts = train_df[\"item_id\"].value_counts()\n", "    id_counts = train_df[\"id\"].value_counts()\n", "    train_df[\"item_freq\"] = train_df[\"item_id\"].map(item_counts).fillna(0).astype(\"float32\")\n", "    val_df[\"item_freq\"] = val_df[\"item_id\"].map(item_counts).fillna(0).astype(\"float32\")\n", "    train_df[\"id_freq\"] = train_df[\"id\"].map(id_counts).fillna(0).astype(\"float32\")\n", "    val_df[\"id_freq\"] = val_df[\"id\"].map(id_counts).fillna(0).astype(\"float32\")\n", "    # Scale freq by train min-max\n", "    for col in [\"item_freq\", \"id_freq\"]:\n", "        mn = train_df[col].min()\n", "        mx = train_df[col].max()\n", "        rng = mx - mn\n", "        if rng == 0:\n", "            train_df[col + \"_scaled\"] = 0.0\n", "            val_df[col + \"_scaled\"] = 0.0\n", "        else:\n", "            train_df[col + \"_scaled\"] = (train_df[col] - mn) / rng\n", "            val_df[col + \"_scaled\"] = (val_df[col] - mn) / rng\n\n", "    # Target encoding (mean encoded on train only)\n", "    global_mean = train_df[TARGET_COL].mean()\n", "    def mean_encode(series: pd.Series, target: pd.Series, alpha: float = 5.0) -> pd.Series:\n", "        counts = series.value_counts()\n", "        means = train_df.groupby(series.name)[TARGET_COL].mean()\n", "        # smoothing: (count * mean + alpha * global_mean) / (count + alpha)\n", "        smoothing = (means * counts + alpha * global_mean) / (counts + alpha)\n", "        return series.map(smoothing)\n", "    train_df[\"item_te\"] = mean_encode(train_df[\"item_id\"], train_df[TARGET_COL])\n", "    val_df[\"item_te\"] = val_df[\"item_id\"].map(train_df.groupby(\"item_id\")[TARGET_COL].mean())\n", "    val_df[\"item_te\"] = val_df[\"item_te\"].fillna(global_mean)\n", "    train_df[\"id_te\"] = mean_encode(train_df[\"id\"], train_df[TARGET_COL])\n", "    val_df[\"id_te\"] = val_df[\"id\"].map(train_df.groupby(\"id\")[TARGET_COL].mean())\n", "    val_df[\"id_te\"] = val_df[\"id_te\"].fillna(global_mean)\n", "    for col in [\"item_te\", \"id_te\"]:\n", "        mn = train_df[col].min()\n", "        mx = train_df[col].max()\n", "        rng = mx - mn\n", "        if rng == 0:\n", "            train_df[col + \"_scaled\"] = 0.0\n", "            val_df[col + \"_scaled\"] = 0.0\n", "        else:\n", "            train_df[col + \"_scaled\"] = (train_df[col] - mn) / rng\n", "            val_df[col + \"_scaled\"] = (val_df[col] - mn) / rng\n\n", "    # Extend feature list with generated cols\n", "    feature_cols = feature_cols + [\n", "        \"item_freq_scaled\",\n", "        \"id_freq_scaled\",\n", "        \"item_te_scaled\",\n", "        \"id_te_scaled\",\n", "    ]\n\n", "    # Binary to int8 to save memory\n", "    for col in BIN_COLS:\n", "        train_df[col] = train_df[col].astype(\"int8\")\n", "        val_df[col] = val_df[col].astype(\"int8\")\n\n", "    # Floats to float32\n", "    for col in NUM_SCALED + CYCLIC:\n", "        train_df[col] = train_df[col].astype(\"float32\")\n", "        val_df[col] = val_df[col].astype(\"float32\")\n", "    return train_df, val_df, feature_cols"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def safe_mape(y_true: np.ndarray, y_pred: np.ndarray, min_denom: float = 1e-3) -> float:\n", "    \"\"\"MAPE that ignores targets near zero to avoid exploding values.\"\"\"\n", "    mask = np.abs(y_true) > min_denom\n", "    if not mask.any():\n", "        return float(\"nan\")\n", "    denom = np.abs(y_true[mask])\n", "    return float(np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n", "    num = np.abs(y_pred - y_true)\n", "    denom = (np.abs(y_true) + np.abs(y_pred)).clip(min=eps)\n", "    return float(np.mean(2.0 * num / denom))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def wrmsse(train_df: pd.DataFrame, val_df: pd.DataFrame, preds: np.ndarray, id_col: str = \"id\") -> float:\n", "    \"\"\"WRMSSE disabled during tuning.\"\"\"\n", "    return 0.0"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_lgbm(train_df: pd.DataFrame, val_df: pd.DataFrame, feature_cols: List[str], params: dict) -> tuple[lgb.LGBMRegressor, dict]:\n", "    X_train = train_df[feature_cols]\n", "    y_train_raw = train_df[TARGET_COL].astype(\"float32\")\n", "    X_val = val_df[feature_cols]\n", "    y_val_raw = val_df[TARGET_COL].astype(\"float32\")\n", "    sample_weight = None\n", "    if TIME_DECAY:\n", "        # weight = exp(-ln(2) * age / half_life); recent days get higher weight\n", "        age = TRAIN_END - train_df[\"d_int\"]\n", "        sample_weight = np.exp(-np.log(2) * age / DECAY_HALF_LIFE).astype(\"float32\")\n", "    if USE_LOG1P_TARGET:\n", "        y_train = np.log1p(y_train_raw)\n", "        y_val = np.log1p(y_val_raw)\n", "    else:\n", "        y_train = y_train_raw\n", "        y_val = y_val_raw\n", "    model = lgb.LGBMRegressor(**params)\n", "    model.fit(\n", "        X_train,\n", "        y_train,\n", "        sample_weight=sample_weight,\n", "        eval_set=[(X_val, y_val)],\n", "        eval_metric=\"rmse\",\n", "        categorical_feature=CAT_COLS,\n", "        callbacks=[lgb.early_stopping(stopping_rounds=EARLY_STOPPING, verbose=50)],\n", "    )\n", "    preds = model.predict(X_val, num_iteration=model.best_iteration_)\n", "    if USE_LOG1P_TARGET:\n", "        preds = np.expm1(preds)\n", "        y_val_used = y_val_raw.values\n", "    else:\n", "        y_val_used = y_val.values\n\n", "    # Some sklearn versions lack squared=False; compute RMSE manually for compatibility.\n", "    rmse = float(np.sqrt(mean_squared_error(y_val_used, preds)))\n", "    mape = safe_mape(y_val_used, preds)\n", "    smape_val = smape(y_val_used, preds)\n", "    wrmsse_val = wrmsse(train_df, val_df, preds)\n", "    metrics = {\"rmse\": rmse, \"mape\": mape, \"smape\": smape_val, \"wrmsse\": wrmsse_val, \"iter\": model.best_iteration_}\n", "    print(\n", "        f\"Validation RMSE: {rmse:.4f} | MAPE (>0.001 mask): {mape:.4f} | SMAPE: {smape_val:.4f} | WRMSSE: {wrmsse_val:.4f}\"\n", "    )\n", "    return model, metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def time_series_folds(df: pd.DataFrame) -> list[tuple[pd.DataFrame, pd.DataFrame]]:\n", "    \"\"\"Create simple time-series folds: fold1 uses train<=TRAIN_END-CV_VAL_LEN, val=last CV_VAL_LEN before TRAIN_END; fold2 uses standard val.\"\"\"\n", "    folds = []\n", "    # fold 1\n", "    val1_start = TRAIN_END - CV_VAL_LEN + 1\n", "    val1_end = TRAIN_END\n", "    train1 = df[df[\"d_int\"] < val1_start]\n", "    val1 = df[(df[\"d_int\"] >= val1_start) & (df[\"d_int\"] <= val1_end)]\n", "    if not val1.empty and len(train1) > 0:\n", "        folds.append((train1, val1))\n", "    # fold 2 (current standard val)\n", "    train2 = df[df[\"d_int\"] <= TRAIN_END]\n", "    val2 = df[(df[\"d_int\"] > TRAIN_END) & (df[\"d_int\"] <= VAL_END)]\n", "    if not val2.empty and len(train2) > 0:\n", "        folds.append((train2, val2))\n", "    return folds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cv_evaluate(df: pd.DataFrame, feature_cols: List[str], params: dict) -> dict:\n", "    folds = time_series_folds(df)\n", "    metrics_list = []\n", "    for i, (tr, va) in enumerate(folds, 1):\n", "        print(f\"  CV fold {i}: train={len(tr):,}, val={len(va):,}\")\n", "        model, metrics = train_lgbm(tr, va, feature_cols, params)\n", "        metrics_list.append(metrics)\n", "    # average metrics\n", "    avg = {k: float(np.nanmean([m[k] for m in metrics_list])) for k in [\"rmse\", \"mape\", \"smape\", \"wrmsse\"]}\n", "    avg[\"iter\"] = int(np.nanmean([m[\"iter\"] for m in metrics_list]))\n", "    return avg"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tweak_params_for_blend(params: dict) -> dict:\n", "    \"\"\"Create a slightly different param set for blending: lower lr, more trees.\"\"\"\n", "    p = params.copy()\n", "    p[\"learning_rate\"] = max(1e-4, params.get(\"learning_rate\", 0.03) * 0.8)\n", "    p[\"n_estimators\"] = int(params.get(\"n_estimators\", 1800) * 1.3)\n", "    return p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tweak_params_for_blend2(params: dict) -> dict:\n", "    \"\"\"Another variant: slightly higher lr, smaller leaves, higher feature fraction.\"\"\"\n", "    p = params.copy()\n", "    p[\"learning_rate\"] = min(0.1, params.get(\"learning_rate\", 0.03) * 1.1)\n", "    p[\"num_leaves\"] = max(31, int(params.get(\"num_leaves\", 255) * 0.8))\n", "    p[\"n_estimators\"] = int(params.get(\"n_estimators\", 1800) * 1.1)\n", "    p[\"feature_fraction\"] = min(1.0, params.get(\"feature_fraction\", 0.8) * 1.05)\n", "    p[\"bagging_fraction\"] = min(1.0, params.get(\"bagging_fraction\", 0.8) * 1.05)\n", "    return p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def random_params(base: dict) -> dict:\n", "    \"\"\"Sample around a base param set to keep search local.\"\"\"\n", "    def jitter(value: float, low_ratio=0.8, high_ratio=1.25):\n", "        return max(1e-4, value * random.uniform(low_ratio, high_ratio))\n", "    def pick_near(value: int, options: list[int]):\n", "        # choose nearest few options around value\n", "        sorted_opts = sorted(options, key=lambda x: abs(x - value))\n", "        return random.choice(sorted_opts[:3])\n", "    p = base.copy()\n", "    p.update(\n", "        num_leaves=pick_near(base.get(\"num_leaves\", 255), [255, 319, 383]),\n", "        learning_rate=jitter(base.get(\"learning_rate\", 0.03), 0.6, 0.9),\n", "        feature_fraction=min(1.0, max(0.6, jitter(base.get(\"feature_fraction\", 0.85), 0.95, 1.1))),\n", "        bagging_fraction=min(1.0, max(0.6, jitter(base.get(\"bagging_fraction\", 0.8), 0.95, 1.1))),\n", "        bagging_freq=random.choice([3, 5, 7]),\n", "        min_data_in_leaf=pick_near(base.get(\"min_data_in_leaf\", 120), [120, 150, 180]),\n", "        lambda_l1=random.choice([0.1, 0.5, 1.0]),\n", "        lambda_l2=random.choice([0.5, 1.0, 2.0]),\n", "        max_depth=random.choice([-1, 8, 10]),\n", "        n_estimators=pick_near(base.get(\"n_estimators\", 1800), [1800, 2000, 2200]),\n", "    )\n", "    return p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main() -> None:\n", "    if LEVEL_MODE == \"state\":\n", "        tasks = [(f\"state {state}\", stores, None, None, False, None) for state, stores in STATE_GROUPS.items()]\n", "    elif LEVEL_MODE == \"store\":\n", "        tasks = []\n", "        for store in STORE_LIST:\n", "            override = None if store in SEARCH_STORES else STORE_PARAMS.get(store)\n", "            tasks.append((f\"store {store}\", [store], None, None, True, override))\n", "    elif LEVEL_MODE == \"dept\":\n", "        tasks = [(\"dept_level\", STORE_LIST, DEPT_FILTER or None, None, False, None)]\n", "    elif LEVEL_MODE == \"item\":\n", "        tasks = [(\"item_level\", STORE_LIST, None, ITEM_FILTER or None, False, None)]\n", "    else:\n", "        raise ValueError(f\"Unknown LEVEL_MODE={LEVEL_MODE}\")\n", "    summary = []\n", "    for label, stores, dept_filter, item_filter, enable_search, override_params in tasks:\n", "        print(f\"\\n===== Training {label} | stores: {stores} from {DATA_DIR} =====\")\n", "        train_df, val_df, feature_cols = build_datasets(stores, dept_filter=dept_filter, item_filter=item_filter)\n", "        print(f\"Train rows: {len(train_df):,} | Val rows: {len(val_df):,}\")\n", "        print(f\"Features: {len(feature_cols)}\")\n", "        do_search = (DO_RANDOM_SEARCH or enable_search) and override_params is None\n", "        if do_search:\n", "            best = None\n", "            base_for_search = override_params or STORE_PARAMS.get(stores[0], BASE_PARAMS)\n", "            for i in range(1, min(N_TRIALS, 10) + 1):\n", "                params = random_params(base_for_search)\n", "                print(f\"\\n=== Trial {i}/{N_TRIALS} params: {params}\")\n", "                model, metrics = train_lgbm(train_df, val_df, feature_cols, params)\n", "                score = metrics[\"rmse\"]\n", "                if (best is None) or (score < best[\"rmse\"]):\n", "                    best = {\n", "                        \"rmse\": score,\n", "                        \"mape\": metrics[\"mape\"],\n", "                        \"smape\": metrics[\"smape\"],\n", "                        \"params\": params,\n", "                        \"iter\": model.best_iteration_,\n", "                    }\n", "            print(\"\\nBest trial:\")\n", "            print(best)\n", "            summary.append({\"label\": label, \"stores\": stores, **best})\n", "        else:\n", "            params = override_params or BASE_PARAMS\n", "            use_cv_here = USE_CV or (stores[0] in CV_STORES)\n", "            if use_cv_here:\n", "                merged_df = pd.concat([train_df, val_df], ignore_index=True)\n", "                metrics = cv_evaluate(merged_df, feature_cols, params)\n", "                print(\n", "                    f\"CV avg -> RMSE {metrics['rmse']:.4f} | MAPE {metrics['mape']:.4f} | SMAPE {metrics['smape']:.4f} | WRMSSE {metrics['wrmsse']:.4f}\"\n", "                )\n", "                result = {\n", "                    \"label\": label,\n", "                    \"stores\": stores,\n", "                    \"rmse\": metrics[\"rmse\"],\n", "                    \"mape\": metrics[\"mape\"],\n", "                    \"smape\": metrics[\"smape\"],\n", "                    \"wrmsse\": metrics[\"wrmsse\"],\n", "                    \"iter\": metrics[\"iter\"],\n", "                    \"params\": params,\n", "                }\n", "                model = None\n", "            else:\n", "                model, metrics = train_lgbm(train_df, val_df, feature_cols, params)\n", "                print(f\"Best iteration: {model.best_iteration_}\")\n", "                result = {\n", "                    \"label\": label,\n", "                    \"stores\": stores,\n", "                    \"rmse\": metrics[\"rmse\"],\n", "                    \"mape\": metrics[\"mape\"],\n", "                    \"smape\": metrics[\"smape\"],\n", "                    \"wrmsse\": metrics[\"wrmsse\"],\n", "                    \"iter\": metrics[\"iter\"],\n", "                    \"params\": params,\n", "                }\n", "            # Optional blending with tweaked params (skip if store in NO_BLEND_STORES)\n", "            if ENABLE_BLEND and (stores[0] not in NO_BLEND_STORES) and not use_cv_here:\n", "                alt_params = tweak_params_for_blend(params)\n", "                alt_model, alt_metrics = train_lgbm(train_df, val_df, feature_cols, alt_params)\n", "                alt_params2 = tweak_params_for_blend2(params)\n", "                alt_model2, alt_metrics2 = train_lgbm(train_df, val_df, feature_cols, alt_params2)\n", "                y_val = val_df[TARGET_COL].values.astype(\"float32\")\n", "                preds_base = model.predict(val_df[feature_cols], num_iteration=model.best_iteration_)\n", "                preds_alt = alt_model.predict(val_df[feature_cols], num_iteration=alt_model.best_iteration_)\n", "                preds_alt2 = alt_model2.predict(val_df[feature_cols], num_iteration=alt_model2.best_iteration_)\n", "                best_blend = None\n", "                # two-model search\n", "                for w in BLEND_WEIGHTS:\n", "                    blended = w * preds_base + (1 - w) * preds_alt\n", "                    rmse_b = float(np.sqrt(mean_squared_error(y_val, blended)))\n", "                    mape_b = safe_mape(y_val, blended)\n", "                    smape_b = smape(y_val, blended)\n", "                    wrmsse_b = wrmsse(train_df, val_df, blended)\n", "                    if (best_blend is None) or (rmse_b < best_blend[\"rmse\"]):\n", "                        best_blend = {\"type\": \"2model\", \"w_base\": w, \"w_alt\": 1 - w, \"rmse\": rmse_b, \"mape\": mape_b, \"smape\": smape_b, \"wrmsse\": wrmsse_b}\n", "                # three-model search (coarse grid ensuring weights sum to 1 and non-negative)\n", "                for wb in [0.2, 0.4, 0.6]:\n", "                    for wa in [0.2, 0.4, 0.6]:\n", "                        wc = 1.0 - wb - wa\n", "                        if wc < 0 or wc > 0.8:\n", "                            continue\n", "                        blended = wb * preds_base + wa * preds_alt + wc * preds_alt2\n", "                        rmse_b = float(np.sqrt(mean_squared_error(y_val, blended)))\n", "                        mape_b = safe_mape(y_val, blended)\n", "                        smape_b = smape(y_val, blended)\n", "                        wrmsse_b = wrmsse(train_df, val_df, blended)\n", "                        if (best_blend is None) or (rmse_b < best_blend[\"rmse\"]):\n", "                            best_blend = {\"type\": \"3model\", \"w_base\": wb, \"w_alt\": wa, \"w_alt2\": wc, \"rmse\": rmse_b, \"mape\": mape_b, \"smape\": smape_b, \"wrmsse\": wrmsse_b}\n", "                print(\n", "                    f\"Blended ({best_blend.get('type')}) RMSE {best_blend['rmse']:.4f} | MAPE {best_blend['mape']:.4f} | SMAPE {best_blend['smape']:.4f} | WRMSSE {best_blend['wrmsse']:.4f} \"\n", "                    f\"(w_base={best_blend.get('w_base')}, w_alt={best_blend.get('w_alt')}, w_alt2={best_blend.get('w_alt2')})\"\n", "                )\n", "                result.update(\n", "                    {\n", "                        \"blend_rmse\": best_blend[\"rmse\"],\n", "                        \"blend_mape\": best_blend[\"mape\"],\n", "                        \"blend_smape\": best_blend[\"smape\"],\n", "                        \"blend_wrmsse\": best_blend[\"wrmsse\"],\n", "                        \"blend_w_base\": best_blend[\"w_base\"],\n", "                        \"blend_w_alt\": best_blend.get(\"w_alt\"),\n", "                        \"blend_w_alt2\": best_blend.get(\"w_alt2\"),\n", "                        \"blend_type\": best_blend.get(\"type\"),\n", "                        \"alt_iter\": alt_model.best_iteration_,\n", "                        \"alt2_iter\": alt_model2.best_iteration_,\n", "                    }\n", "                )\n", "            # Persist incremental result to disk to avoid reruns on later failures\n", "            try:\n", "                SUMMARY_PATH.parent.mkdir(parents=True, exist_ok=True)\n", "                with SUMMARY_PATH.open(\"a\", encoding=\"utf-8\") as f:\n", "                    f.write(json.dumps(result, ensure_ascii=False))\n", "                    f.write(\"\\n\")\n", "            except Exception as e:\n", "                print(f\"Warning: failed to write summary log ({e})\")\n", "            summary.append(result)\n", "    print(\"\\n===== Summary per task =====\")\n", "    for row in summary:\n", "        print(row)\n\n", "    # Save full summary and blend weights for later reuse\n", "    try:\n", "        with SUMMARY_PATH.open(\"w\", encoding=\"utf-8\") as f:\n", "            json.dump(summary, f, ensure_ascii=False, indent=2)\n", "        blend_weights = {}\n", "        for row in summary:\n", "            store_key = row[\"stores\"][0] if row.get(\"stores\") else row.get(\"label\")\n", "            if row.get(\"blend_type\"):\n", "                blend_weights[store_key] = {\n", "                    \"type\": row.get(\"blend_type\"),\n", "                    \"w_base\": row.get(\"blend_w_base\", 1.0),\n", "                    \"w_alt\": row.get(\"blend_w_alt\", 0.0),\n", "                    \"w_alt2\": row.get(\"blend_w_alt2\", 0.0),\n", "                }\n", "            else:\n", "                blend_weights[store_key] = {\"type\": \"none\", \"w_base\": 1.0, \"w_alt\": 0.0, \"w_alt2\": 0.0}\n", "        with BLEND_WEIGHT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n", "            json.dump(blend_weights, f, ensure_ascii=False, indent=2)\n", "        print(f\"\\nSaved summary to {SUMMARY_PATH} and blend weights to {BLEND_WEIGHT_PATH}\")\n", "    except Exception as e:\n", "        print(f\"Warning: failed to save summary/weights ({e})\")"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}