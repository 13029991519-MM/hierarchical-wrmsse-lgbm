{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Iterative 28-day forecast using trained LGBM models and stored blend weights.<br>\n", "This is a simplified scaffold: it trains on full history (d_11941) per store,<br>\n", "then rolls forward day by day to generate future features and predictions.<br>\n", "Inputs:<br>\n", "- newfinaldata/processed_*.csv : historical features up to d_1941 (per store)<br>\n", "- data/calendar.csv, data/sell_prices.csv : calendar/price info for future days<br>\n", "- weight/blend_weights_auto.json : per-store blend weights; stores in NO_BLEND_STORES use single model<br>\n", "Output:<br>\n", "- submission.csv saved under future_finaldata/ (28-day forecasts, one row per id)<br>\n", "Note: This is a lightweight implementation intended to give a working pipeline;<br>\n", "for production, you may want to optimize memory and add more safety checks.<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import annotations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import gc\n", "import json\n", "import os\n", "from pathlib import Path\n", "from typing import Dict, List"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\n", "import numpy as np\n", "import pandas as pd"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATA_DIR = Path(\"newfinaldata\")\n", "CAL_PATH = Path(\"data/calendar.csv\")\n", "PRICE_PATH = Path(\"data/sell_prices.csv\")\n", "BLEND_PATH = Path(\"weight/blend_weights_auto.json\")\n", "ALT_BLEND_PATH = Path(\"weight_v2/delay_120_weight_v2.json\")\n", "OUT_DIR = Path(\"future_finaldata\")\n", "OUT_DIR.mkdir(exist_ok=True, parents=True)\n", "CACHE_DIR = OUT_DIR / \"cache\"\n", "CACHE_DIR.mkdir(exist_ok=True, parents=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_args() -> argparse.Namespace:\n", "    parser = argparse.ArgumentParser(description=\"Iterative forecast for evaluation submission (only TX_1/TX_2 by default).\")\n", "    parser.add_argument(\n", "        \"--stores\",\n", "        type=str,\n", "        default=\"TX_1,TX_2\",\n", "        help=\"Comma-separated stores to forecast (default: TX_1,TX_2).\",\n", "    )\n", "    parser.add_argument(\n", "        \"--out\",\n", "        type=Path,\n", "        default=Path(\"future_finaldata/submission_tx12_cmodel.csv\"),\n", "        help=\"Path for the new submission output (won't override submission.csv).\",\n", "    )\n", "    return parser.parse_args()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Columns (must match training)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["NUM_SCALED = [\n", "    \"sell_price_scaled\",\n", "    \"baseline_price_scaled\",\n", "    \"discount_scaled\",\n", "    \"promo_intensity_scaled\",\n", "    \"lag_7_scaled\",\n", "    \"lag_30_scaled\",\n", "    \"rolling_mean_7_scaled\",\n", "    \"rolling_mean_30_scaled\",\n", "    \"rolling_std_7_scaled\",\n", "    \"rolling_std_30_scaled\",\n", "    \"lag_1_scaled\",\n", "    \"lag_14_scaled\",\n", "    \"lag_28_scaled\",\n", "    \"lag_56_scaled\",\n", "    \"lag_84_scaled\",\n", "    \"rolling_mean_14_scaled\",\n", "    \"rolling_mean_28_scaled\",\n", "    \"rolling_mean_56_scaled\",\n", "    \"rolling_mean_84_scaled\",\n", "    \"rolling_std_14_scaled\",\n", "    \"rolling_std_28_scaled\",\n", "    \"rolling_std_56_scaled\",\n", "    \"rolling_std_84_scaled\",\n", "    \"price_ratio_scaled\",\n", "    \"discount_pct_scaled\",\n", "    \"snap_wday_scaled\",\n", "    \"promo_holiday_scaled\",\n", "    \"promo_wday_sin_scaled\",\n", "    \"promo_wday_cos_scaled\",\n", "    \"discount_snap_scaled\",\n", "    \"item_freq_scaled\",\n", "    \"id_freq_scaled\",\n", "    \"item_te_scaled\",\n", "    \"id_te_scaled\",\n", "    # rolling median/min/max (scaled)\n", "    \"rolling_median_7_scaled\",\n", "    \"rolling_median_14_scaled\",\n", "    \"rolling_median_28_scaled\",\n", "    \"rolling_median_30_scaled\",\n", "    \"rolling_median_56_scaled\",\n", "    \"rolling_median_84_scaled\",\n", "    \"rolling_min_7_scaled\",\n", "    \"rolling_min_14_scaled\",\n", "    \"rolling_min_28_scaled\",\n", "    \"rolling_min_30_scaled\",\n", "    \"rolling_min_56_scaled\",\n", "    \"rolling_min_84_scaled\",\n", "    \"rolling_max_7_scaled\",\n", "    \"rolling_max_14_scaled\",\n", "    \"rolling_max_28_scaled\",\n", "    \"rolling_max_30_scaled\",\n", "    \"rolling_max_56_scaled\",\n", "    \"rolling_max_84_scaled\",\n", "    # price momentum / z-score (scaled)\n", "    \"sell_price_week_chg_scaled\",\n", "    \"sell_price_month_chg_scaled\",\n", "    \"sell_price_z28_scaled\",\n", "    \"discount_week_chg_scaled\",\n", "    \"discount_month_chg_scaled\",\n", "    \"discount_z28_scaled\",\n", "    # streaks / holiday distance (scaled)\n", "    \"promo_streak_scaled\",\n", "    \"holiday_streak_scaled\",\n", "    \"days_since_holiday_scaled\",\n", "    \"days_until_holiday_scaled\",\n", "]\n", "# raw numeric columns used for scaling/lag creation\n", "RAW_BASE_NUM = [\n", "    \"sell_price\",\n", "    \"baseline_price\",\n", "    \"discount\",\n", "    \"promo_intensity\",\n", "    \"price_ratio\",\n", "    \"discount_pct\",\n", "    \"snap_wday\",\n", "    \"promo_holiday\",\n", "    \"promo_wday_sin\",\n", "    \"promo_wday_cos\",\n", "    \"discount_snap\",\n", "    \"promo_streak\",\n", "    \"holiday_streak\",\n", "    \"days_since_holiday\",\n", "    \"days_until_holiday\",\n", "    \"sell_price_week_chg\",\n", "    \"sell_price_month_chg\",\n", "    \"sell_price_z28\",\n", "    \"discount_week_chg\",\n", "    \"discount_month_chg\",\n", "    \"discount_z28\",\n", "]\n", "RAW_LAG_COLS = [f\"lag_{k}\" for k in [1, 7, 14, 28, 30, 56, 84]]\n", "RAW_ROLL_MEAN = [f\"rolling_mean_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "RAW_ROLL_STD = [f\"rolling_std_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "RAW_ROLL_MED = [f\"rolling_median_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "RAW_ROLL_MIN = [f\"rolling_min_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "RAW_ROLL_MAX = [f\"rolling_max_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "CYCLIC = [\"wday_sin\", \"wday_cos\", \"month_sin\", \"month_cos\", \"quarter_sin\", \"quarter_cos\"]\n", "BIN_COLS = [\"snap_CA\", \"snap_TX\", \"snap_WI\", \"IsHoliday\", \"IsPromotion\"]\n", "CAT_COLS = [\n", "    \"state_id\",\n", "    \"store_id\",\n", "    \"cat_id\",\n", "    \"dept_id\",\n", "    \"event_name_1\",\n", "    \"event_type_1\",\n", "    \"event_name_2\",\n", "    \"event_type_2\",\n", "    \"item_id\",\n", "    \"id\",\n", "]\n", "TARGET_COL = \"sales\"\n", "NO_BLEND_STORES = set()  # \u947b\u30e5\u53cf\u6434\u6940\u5d1f\u59af\u2605\u7d1d main \u7455\u55d9\u6d0a\n", "# Parallel flag; set False if memory is tight\n", "PARALLEL = False\n", "MAX_WORKERS = max(1, min(4, os.cpu_count() or 1))\n", "# History window (days) to keep per store for faster inference; None=all\n", "HISTORY_WINDOW = None  # use full history; set to int (e.g., 500) to truncate"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_blend_weights() -> Dict[str, dict]:\n", "    path = BLEND_PATH if BLEND_PATH.exists() else ALT_BLEND_PATH\n", "    if not path.exists():\n", "        raise FileNotFoundError(f\"Blend weights not found at {BLEND_PATH} or {ALT_BLEND_PATH}\")\n", "    with open(path, \"r\", encoding=\"utf-8\") as f:\n", "        return json.load(f)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_store_history(store: str) -> pd.DataFrame:\n", "    path = DATA_DIR / f\"processed_{store}.csv\"\n", "    if not path.exists():\n", "        raise FileNotFoundError(path)\n", "    cols = pd.read_csv(path, nrows=0).columns.tolist()\n", "    wanted = (\n", "        NUM_SCALED\n", "        + RAW_BASE_NUM\n", "        + RAW_LAG_COLS\n", "        + RAW_ROLL_MEAN\n", "        + RAW_ROLL_STD\n", "        + RAW_ROLL_MED\n", "        + RAW_ROLL_MIN\n", "        + RAW_ROLL_MAX\n", "        + CYCLIC\n", "        + BIN_COLS\n", "        + CAT_COLS\n", "        + [TARGET_COL, \"d\"]\n", "    )\n", "    usecols = [c for c in wanted if c in cols]\n", "    dtypes = {c: \"float32\" for c in NUM_SCALED if c in usecols}\n", "    dtypes.update({c: \"float32\" for c in RAW_BASE_NUM + RAW_LAG_COLS + RAW_ROLL_MEAN + RAW_ROLL_STD + RAW_ROLL_MED + RAW_ROLL_MIN + RAW_ROLL_MAX if c in usecols})\n", "    dtypes.update({c: \"int8\" for c in BIN_COLS if c in usecols})\n", "    dtypes.update({c: \"category\" for c in CAT_COLS if c in usecols})\n", "    if TARGET_COL in usecols:\n", "        dtypes[TARGET_COL] = \"float32\"\n", "    df = pd.read_csv(path, usecols=usecols, dtype=dtypes, engine=\"pyarrow\")\n", "    for col in wanted:\n", "        if col not in df.columns:\n", "            if col in BIN_COLS:\n", "                df[col] = np.int8(0)\n", "            elif col in CAT_COLS:\n", "                df[col] = pd.Series([\"Unknown\"] * len(df), dtype=\"category\")\n", "            else:\n", "                df[col] = np.float32(0.0)\n", "    num_cols = [c for c in wanted if c not in CAT_COLS + BIN_COLS + [\"d\"]]\n", "    for col in num_cols:\n", "        df[col] = pd.to_numeric(df[col], errors=\"coerce\").astype(\"float32\").fillna(0)\n", "    for col in BIN_COLS:\n", "        df[col] = df[col].astype(\"int8\")\n", "    for col in CAT_COLS:\n", "        df[col] = df[col].astype(\"category\")\n", "    df[\"d_int\"] = df[\"d\"].str.replace(\"d_\", \"\", regex=False).astype(int)\n", "    if any(c not in df.columns for c in [\"item_freq_scaled\", \"id_freq_scaled\", \"item_te_scaled\", \"id_te_scaled\"]):\n", "        item_counts = df[\"item_id\"].value_counts()\n", "        id_counts = df[\"id\"].value_counts()\n", "        item_means = df.groupby(\"item_id\")[TARGET_COL].mean()\n", "        id_means = df.groupby(\"id\")[TARGET_COL].mean()\n", "        global_mean = df[TARGET_COL].mean()\n", "        df[\"item_freq\"] = df[\"item_id\"].map(item_counts).fillna(0).astype(\"float32\")\n", "        df[\"id_freq\"] = df[\"id\"].map(id_counts).fillna(0).astype(\"float32\")\n", "        df[\"item_te\"] = df[\"item_id\"].map(item_means).fillna(global_mean).astype(\"float32\")\n", "        df[\"id_te\"] = df[\"id\"].map(id_means).fillna(global_mean).astype(\"float32\")\n", "        for col in [\"item_freq\", \"id_freq\", \"item_te\", \"id_te\"]:\n", "            mn = df[col].min()\n", "            mx = df[col].max()\n", "            rng = mx - mn\n", "            df[col + \"_scaled\"] = 0.0 if rng == 0 else (df[col] - mn) / rng\n", "        for col in [\"item_freq\", \"id_freq\", \"item_te\", \"id_te\", \"item_freq_scaled\", \"id_freq_scaled\", \"item_te_scaled\", \"id_te_scaled\"]:\n", "            df[col] = df[col].astype(\"float32\")\n", "    if HISTORY_WINDOW is not None:\n", "        cutoff = df[\"d_int\"].max() - HISTORY_WINDOW + 1\n", "        df = df[df[\"d_int\"] >= cutoff].reset_index(drop=True)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def fit_models(train_df: pd.DataFrame, feature_cols: List[str], params: dict):\n", "    model = lgb.LGBMRegressor(**params)\n", "    model.fit(\n", "        train_df[feature_cols],\n", "        train_df[TARGET_COL].astype(\"float32\"),\n", "        categorical_feature=CAT_COLS,\n", "    )\n", "    return model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prepare_future_calendar(calendar: pd.DataFrame) -> pd.DataFrame:\n", "    # create sin/cos for future rows\n", "    cal = calendar.copy()\n", "    cal[\"d_int\"] = cal[\"d\"].str.replace(\"d_\", \"\", regex=False).astype(int)\n", "    if \"quarter\" not in cal.columns:\n", "        cal[\"quarter\"] = ((cal[\"month\"] - 1) // 3 + 1).astype(int)\n", "    cal[\"wday_sin\"] = np.sin(2 * np.pi * cal[\"wday\"] / 7)\n", "    cal[\"wday_cos\"] = np.cos(2 * np.pi * cal[\"wday\"] / 7)\n", "    cal[\"month_sin\"] = np.sin(2 * np.pi * cal[\"month\"] / 12)\n", "    cal[\"month_cos\"] = np.cos(2 * np.pi * cal[\"month\"] / 12)\n", "    cal[\"quarter_sin\"] = np.sin(2 * np.pi * cal[\"quarter\"] / 4)\n", "    cal[\"quarter_cos\"] = np.cos(2 * np.pi * cal[\"quarter\"] / 4)\n", "    return cal"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def iterative_forecast(\n", "    store: str,\n", "    blend_cfg: dict,\n", "    params: dict,\n", "    cal: pd.DataFrame,\n", "    prices_store: pd.DataFrame,\n", ") -> pd.DataFrame:\n", "    df = load_store_history(store)\n", "    last_d = df[\"d_int\"].max()\n", "    horizon = 28\n\n", "    # Precompute encoders for freq/te\n", "    item_counts = df[\"item_id\"].value_counts()\n", "    id_counts = df[\"id\"].value_counts()\n", "    global_mean = df[TARGET_COL].mean()\n", "    item_means = df.groupby(\"item_id\")[TARGET_COL].mean()\n", "    id_means = df.groupby(\"id\")[TARGET_COL].mean()\n", "    # min-max stats for freq/te from train_df\n", "    item_freq_min, item_freq_max = item_counts.min(), item_counts.max()\n", "    id_freq_min, id_freq_max = id_counts.min(), id_counts.max()\n", "    item_te_min, item_te_max = item_means.min(), item_means.max()\n", "    id_te_min, id_te_max = id_means.min(), id_means.max()\n", "    feature_cols = NUM_SCALED + CYCLIC + BIN_COLS + CAT_COLS\n", "    train_df = df[df[\"d_int\"] <= last_d].copy()\n", "    # ensure raw lag/rolling cols exist for scaling stats\n", "    raw_lags = [f\"lag_{k}\" for k in [1, 7, 14, 28, 30, 56, 84]]\n", "    raw_roll_mean = [f\"rolling_mean_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "    raw_roll_std = [f\"rolling_std_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "    raw_roll_med = [f\"rolling_median_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "    raw_roll_min = [f\"rolling_min_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "    raw_roll_max = [f\"rolling_max_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "    for col in raw_lags + raw_roll_mean + raw_roll_std + raw_roll_med + raw_roll_min + raw_roll_max:\n", "        if col not in train_df.columns:\n", "            train_df[col] = np.float32(0.0)\n", "    # set categorical dtypes based on train\n", "    cat_types = {}\n", "    for col in CAT_COLS:\n", "        cats = pd.CategoricalDtype(categories=train_df[col].dropna().unique())\n", "        train_df[col] = train_df[col].astype(cats)\n", "        cat_types[col] = cats\n", "    # binary to int\n", "    for col in BIN_COLS:\n", "        train_df[col] = train_df[col].astype(\"int8\")\n", "    # ensure freq/te scaled exist in train_df for min-max reference\n", "    if \"item_freq_scaled\" not in train_df:\n", "        train_df[\"item_freq_scaled\"] = item_counts.reindex(train_df[\"item_id\"]).values\n", "        train_df[\"id_freq_scaled\"] = id_counts.reindex(train_df[\"id\"]).values\n", "        train_df[\"item_te_scaled\"] = item_means.reindex(train_df[\"item_id\"]).fillna(global_mean).values\n", "        train_df[\"id_te_scaled\"] = id_means.reindex(train_df[\"id\"]).fillna(global_mean).values\n\n", "    # Fit models\n", "    model_base = fit_models(train_df, feature_cols, params)\n", "    models = [model_base]\n", "    if store not in NO_BLEND_STORES:\n", "        from train_lgbm_baseline import tweak_params_for_blend, tweak_params_for_blend2  # reuse helpers\n", "        model_alt = fit_models(train_df, feature_cols, tweak_params_for_blend(params))\n", "        model_alt2 = fit_models(train_df, feature_cols, tweak_params_for_blend2(params))\n", "        models.extend([model_alt, model_alt2])\n", "    preds_records = []\n", "    cal_future = cal[cal[\"d_int\"] > last_d].copy()\n\n", "    # Build a dict for quick row updates\n", "    df.set_index(\"d_int\", inplace=True)\n", "    for d_int in range(last_d + 1, last_d + horizon + 1):\n", "        # get calendar row\n", "        cal_row = cal_future[cal_future[\"d_int\"] == d_int]\n", "        if cal_row.empty:\n", "            raise ValueError(f\"Calendar missing d_{d_int}\")\n", "        cal_row = cal_row.iloc[0]\n", "        # price info: merge on (item_id, wm_yr_wk)\n", "        week = cal_row[\"wm_yr_wk\"]\n", "        price_map = prices_store[prices_store[\"wm_yr_wk\"] == week].set_index(\"item_id\")\n\n", "        # build today's rows per id\n", "        todays = []\n", "        for id_val, hist_row in df[df.index == last_d].iterrows():\n", "            item_id = hist_row[\"item_id\"]\n", "            row = hist_row.copy()\n", "            row.name = d_int\n", "            row[\"d\"] = f\"d_{d_int}\"\n", "            row[\"d_int\"] = d_int\n", "            # calendar fields\n", "            for col in [\"wday\", \"month\", \"year\", \"quarter\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\", \"wday_sin\", \"wday_cos\", \"month_sin\", \"month_cos\", \"quarter_sin\", \"quarter_cos\"]:\n", "                row[col] = cal_row.get(col, np.nan)\n", "            # derive holiday/promo flags if not present in calendar\n", "            row[\"IsHoliday\"] = cal_row.get(\n", "                \"IsHoliday\",\n", "                1\n", "                if (pd.notna(cal_row.get(\"event_name_1\", np.nan)) or pd.notna(cal_row.get(\"event_name_2\", np.nan)))\n", "                else 0,\n", "            )\n", "            row[\"IsPromotion\"] = cal_row.get(\"IsPromotion\", 0)\n", "            # price fields\n", "            if item_id in price_map.index:\n", "                row[\"sell_price\"] = price_map.loc[item_id, \"sell_price\"]\n", "                row[\"baseline_price\"] = price_map.loc[item_id, \"sell_price\"]  # assume baseline ~ sell_price if missing\n", "            else:\n", "                row[\"sell_price\"] = row.get(\"sell_price\", 0)\n", "                row[\"baseline_price\"] = row.get(\"baseline_price\", 0)\n", "            row[\"discount\"] = max(0.0, row[\"baseline_price\"] - row[\"sell_price\"])\n", "            row[\"promo_intensity\"] = row[\"discount\"] * row[\"IsPromotion\"]\n", "            row[\"price_ratio\"] = row[\"sell_price\"] / row[\"baseline_price\"] if row[\"baseline_price\"] > 0 else 0\n", "            row[\"discount_pct\"] = row[\"discount\"] / row[\"baseline_price\"] if row[\"baseline_price\"] > 0 else 0\n", "            snap_state = row[\"snap_CA\"] if row[\"state_id\"] == \"CA\" else row[\"snap_TX\"] if row[\"state_id\"] == \"TX\" else row[\"snap_WI\"]\n", "            row[\"snap_wday\"] = snap_state * row[\"wday\"]\n", "            row[\"promo_holiday\"] = row[\"IsPromotion\"] * row[\"IsHoliday\"]\n", "            row[\"promo_wday_sin\"] = row[\"promo_intensity\"] * row[\"wday_sin\"]\n", "            row[\"promo_wday_cos\"] = row[\"promo_intensity\"] * row[\"wday_cos\"]\n", "            row[\"discount_snap\"] = row[\"discount_pct\"] * snap_state\n", "            todays.append(row)\n", "        today_df = pd.DataFrame(todays)\n", "        today_df.set_index(\"d_int\", inplace=True)\n\n", "        # compute lags/rollings using existing df (which includes past preds)\n", "        df_full = pd.concat([df, today_df]).sort_index()\n", "        g = df_full.groupby(\"id\")[TARGET_COL]\n", "        for lag in [1, 7, 14, 28, 30, 56, 84]:\n", "            df_full[f\"lag_{lag}\"] = g.shift(lag)\n", "        for win in [7, 14, 28, 30, 56, 84]:\n", "            df_full[f\"rolling_mean_{win}\"] = g.transform(lambda s, w=win: s.shift(1).rolling(w, min_periods=1).mean())\n", "            df_full[f\"rolling_std_{win}\"] = g.transform(lambda s, w=win: s.shift(1).rolling(w, min_periods=1).std())\n", "            df_full[f\"rolling_median_{win}\"] = g.transform(lambda s, w=win: s.shift(1).rolling(w, min_periods=1).median())\n", "            df_full[f\"rolling_min_{win}\"] = g.transform(lambda s, w=win: s.shift(1).rolling(w, min_periods=1).min())\n", "            df_full[f\"rolling_max_{win}\"] = g.transform(lambda s, w=win: s.shift(1).rolling(w, min_periods=1).max())\n\n", "        # take current day rows\n", "        cur = df_full.loc[[d_int]].reset_index()\n\n", "        # freq/te (use train stats)\n", "        cur[\"item_freq\"] = cur[\"item_id\"].map(item_counts).fillna(0)\n", "        cur[\"id_freq\"] = cur[\"id\"].map(id_counts).fillna(0)\n", "        cur[\"item_te\"] = cur[\"item_id\"].map(item_means).fillna(global_mean)\n", "        cur[\"id_te\"] = cur[\"id\"].map(id_means).fillna(global_mean)\n", "        # min-max scaling using train stats\n", "        cur[\"item_freq_scaled\"] = 0.0 if item_freq_max == item_freq_min else (cur[\"item_freq\"] - item_freq_min) / (item_freq_max - item_freq_min)\n", "        cur[\"id_freq_scaled\"] = 0.0 if id_freq_max == id_freq_min else (cur[\"id_freq\"] - id_freq_min) / (id_freq_max - id_freq_min)\n", "        cur[\"item_te_scaled\"] = 0.0 if item_te_max == item_te_min else (cur[\"item_te\"] - item_te_min) / (item_te_max - item_te_min)\n", "        cur[\"id_te_scaled\"] = 0.0 if id_te_max == id_te_min else (cur[\"id_te\"] - id_te_min) / (id_te_max - id_te_min)\n\n", "        # scale new numeric cols using train min-max\n", "        base_scale_cols = [\n", "            \"sell_price\",\n", "            \"baseline_price\",\n", "            \"discount\",\n", "            \"promo_intensity\",\n", "            \"price_ratio\",\n", "            \"discount_pct\",\n", "            \"snap_wday\",\n", "            \"promo_holiday\",\n", "            \"promo_wday_sin\",\n", "            \"promo_wday_cos\",\n", "            \"discount_snap\",\n", "            \"promo_streak\",\n", "            \"holiday_streak\",\n", "            \"days_since_holiday\",\n", "            \"days_until_holiday\",\n", "            \"sell_price_week_chg\",\n", "            \"sell_price_month_chg\",\n", "            \"sell_price_z28\",\n", "            \"discount_week_chg\",\n", "            \"discount_month_chg\",\n", "            \"discount_z28\",\n", "        ]\n", "        lag_cols = [f\"lag_{k}\" for k in [1, 7, 14, 28, 30, 56, 84]]\n", "        roll_mean_cols = [f\"rolling_mean_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "        roll_std_cols = [f\"rolling_std_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "        roll_median_cols = [f\"rolling_median_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "        roll_min_cols = [f\"rolling_min_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "        roll_max_cols = [f\"rolling_max_{k}\" for k in [7, 14, 28, 30, 56, 84]]\n", "        for col in base_scale_cols + lag_cols + roll_mean_cols + roll_std_cols + roll_median_cols + roll_min_cols + roll_max_cols:\n", "            mn = train_df[col].min(skipna=True)\n", "            mx = train_df[col].max(skipna=True)\n", "            rng = mx - mn\n", "            cur[f\"{col}_scaled\"] = 0.0 if pd.isna(rng) or rng == 0 else (cur[col].fillna(0) - mn) / rng\n\n", "        # set dtypes consistent with train\n", "        for col in CAT_COLS:\n", "            cur[col] = cur[col].astype(cat_types[col])\n", "        for col in BIN_COLS:\n", "            cur[col] = cur[col].astype(\"int8\")\n\n", "        # predict\n", "        cur_feats = cur[feature_cols]\n", "        preds = []\n", "        if store in NO_BLEND_STORES:\n", "            preds = model_base.predict(cur_feats, num_iteration=model_base.best_iteration_)\n", "        else:\n", "            p0 = models[0].predict(cur_feats, num_iteration=models[0].best_iteration_)\n", "            p1 = models[1].predict(cur_feats, num_iteration=models[1].best_iteration_)\n", "            p2 = models[2].predict(cur_feats, num_iteration=models[2].best_iteration_)\n", "            w = blend_cfg.get(store, {\"w_base\": 1.0, \"w_alt\": 0.0, \"w_alt2\": 0.0})\n", "            w_base = float(w.get(\"w_base\", 1.0) or 0.0)\n", "            w_alt = float(w.get(\"w_alt\", 0.0) or 0.0)\n", "            w_alt2 = float(w.get(\"w_alt2\", 0.0) or 0.0)\n", "            preds = w_base * p0 + w_alt * p1 + w_alt2 * p2\n", "        cur[\"sales\"] = preds\n", "        cur = cur.set_index(\"d_int\")\n", "        # keep full columns for next-step lag/rolling computation\n", "        df = pd.concat([df, cur[df.columns]], sort=False)\n", "        preds_records.append(cur.reset_index()[[\"id\", \"d\", \"sales\"]].rename(columns={\"sales\": \"pred\"}))\n", "    preds_df = pd.concat(preds_records, ignore_index=True)\n", "    return preds_df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def forecast_store(args) -> pd.DataFrame:\n", "    store, blend_cfg, params, cal, prices_store = args\n", "    return iterative_forecast(store, blend_cfg, params, cal, prices_store)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    args = parse_args()\n", "    blend_cfg_all = load_blend_weights()\n", "    requested = [s.strip() for s in args.stores.split(\",\") if s.strip()]\n", "    if not requested:\n", "        raise ValueError(\"No stores provided for forecasting.\")\n", "    blend_cfg = {store: blend_cfg_all[store] for store in requested if store in blend_cfg_all}\n", "    missing = [store for store in requested if store not in blend_cfg_all]\n", "    if missing:\n", "        print(f\"Warning: blend config missing for {missing}; they will be skipped.\")\n", "    if not blend_cfg:\n", "        raise ValueError(\"None of the requested stores have blend weights available.\")\n", "    for store in blend_cfg:\n", "        blend_cfg[store] = {\"type\": \"c_model\", \"w_base\": 0.0, \"w_alt\": 1.0, \"w_alt2\": 0.0}\n", "    calendar = pd.read_csv(CAL_PATH)\n", "    calendar = prepare_future_calendar(calendar)\n", "    prices = pd.read_csv(PRICE_PATH)\n", "    # split prices by store to avoid repeated filtering\n", "    prices_dict = {sid: df for sid, df in prices.groupby(\"store_id\")}\n", "    all_preds = []\n", "    jobs = []\n", "    from train_lgbm_baseline import STORE_PARAMS\n", "    for store in blend_cfg.keys():\n", "        params = STORE_PARAMS.get(store)\n", "        if params is None:\n", "            continue\n", "        prices_store = prices_dict.get(store)\n", "        jobs.append((store, blend_cfg, params, calendar, prices_store))\n", "    def run_job(job):\n", "        store, blend_cfg_i, params_i, cal_i, prices_i = job\n", "        cache_path = CACHE_DIR / f\"preds_{store}.parquet\"\n", "        if cache_path.exists():\n", "            print(f\"Loading cached predictions for {store}\")\n", "            return cache_path\n", "        print(f\"Forecasting {store} ...\")\n", "        preds_df = iterative_forecast(\n", "            store,\n", "            blend_cfg_i,\n", "            params_i,\n", "            cal_i,\n", "            prices_i,\n", "        )\n", "        preds_df.to_parquet(cache_path, index=False)\n", "        print(f\"Done {store}\")\n", "        return cache_path\n\n", "    # sequential to save memory\n", "    cache_files = []\n", "    for job in jobs:\n", "        path = run_job(job)\n", "        cache_files.append(path)\n", "        gc.collect()\n", "    sub = pd.concat((pd.read_parquet(p) for p in cache_files), ignore_index=True)\n", "    # keep only necessary columns and downcast to save memory\n", "    sub = sub[[\"id\", \"d\", \"pred\"]]\n", "    sub[\"pred\"] = pd.to_numeric(sub[\"pred\"], errors=\"coerce\").astype(\"float32\").fillna(0)\n", "    sub[\"d_num\"] = sub[\"d\"].str.replace(\"d_\", \"\", regex=False).astype(\"int16\")\n", "    # Pivot to submission wide format (evaluation rows only, float32 to save memory)\n", "    pivot = (\n", "        sub[sub[\"id\"].str.endswith(\"evaluation\")][[\"id\", \"d_num\", \"pred\"]]\n", "        .pivot_table(index=\"id\", columns=\"d_num\", values=\"pred\", aggfunc=\"first\")\n", "        .astype(\"float32\")\n", "        .sort_index(axis=1)\n", "    )\n", "    max_d = pivot.columns.max()\n", "    horizon = list(range(max_d - 27, max_d + 1))\n", "    pivot = pivot[horizon]\n", "    pivot.columns = [f\"F{i}\" for i in range(1, 29)]\n", "    pivot.reset_index(inplace=True)\n", "    sample = pd.read_csv(\"data/sample_submission.csv\")\n", "    value_cols = [c for c in sample.columns if c.startswith(\"F\")]\n", "    sample[value_cols] = sample[value_cols].astype(\"float32\")\n", "    sample = sample.set_index(\"id\")\n", "    preds_aligned = pivot.set_index(\"id\")\n", "    eval_ids = [idx for idx in sample.index if idx.endswith(\"evaluation\") and idx in preds_aligned.index]\n", "    sample.loc[eval_ids, value_cols] = preds_aligned.loc[eval_ids, value_cols].values\n", "    sample.reset_index(inplace=True)\n", "    args.out.parent.mkdir(parents=True, exist_ok=True)\n", "    sample.to_csv(args.out, index=False)\n", "    print(f\"Wrote submission to {args.out} (aligned to sample_submission)\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}