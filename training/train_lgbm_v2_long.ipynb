{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Train a long-seasonality LightGBM model on processed_v2 features (includes lag/roll 91/182/365, price stats).<br>\n", "Validates on d_1914\u9225\u63f9_1941 for selected stores (default CA_1, TX_1, WI_1).<br>\n", "Outputs metrics to weight_v2/summary_v2_long.json (does not overwrite baseline).<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import annotations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import json\n", "from pathlib import Path\n", "from typing import List"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["TARGET_COL = \"sales\"\n", "DATA_DIR = Path(\"processed_v2\")\n", "OUT_DIR = Path(\"weight_v2\")\n", "OUT_DIR.mkdir(parents=True, exist_ok=True)\n", "SUMMARY_PATH = OUT_DIR / \"summary_v2_long.json\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STORES = [\"CA_1\", \"TX_1\", \"WI_1\"]\n", "TRAIN_END = 1913\n", "VAL_END = 1941"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Model params (conservative)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["PARAMS = dict(\n", "    objective=\"regression\",\n", "    metric=[\"rmse\", \"mape\"],\n", "    learning_rate=0.05,\n", "    num_leaves=127,\n", "    max_depth=8,\n", "    feature_fraction=0.85,\n", "    bagging_fraction=0.85,\n", "    bagging_freq=5,\n", "    min_data_in_leaf=300,\n", "    lambda_l1=0.5,\n", "    lambda_l2=1.0,\n", "    n_estimators=1200,\n", "    max_bin=255,\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def safe_mape(y_true: np.ndarray, y_pred: np.ndarray, min_denom: float = 1e-3) -> float:\n", "    mask = np.abs(y_true) > min_denom\n", "    if not mask.any():\n", "        return float(\"nan\")\n", "    denom = np.abs(y_true[mask])\n", "    return float(np.mean(np.abs(y_true[mask] - y_pred[mask]) / denom))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_store(store: str) -> pd.DataFrame:\n", "    path = DATA_DIR / f\"processed_{store}.csv\"\n", "    if not path.exists():\n", "        raise FileNotFoundError(path)\n", "    df = pd.read_csv(path)\n", "    df[\"d_int\"] = df[\"d_int\"].astype(int)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_datasets(store: str) -> tuple[pd.DataFrame, pd.DataFrame, List[str], List[str]]:\n", "    df = read_store(store)\n", "    cat_cols = [\"state_id\", \"store_id\", \"cat_id\", \"dept_id\", \"item_id\", \"id\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n", "    bin_cols = [c for c in df.columns if c in [\"snap_CA\", \"snap_TX\", \"snap_WI\", \"IsHoliday\", \"IsPromotion\"]]\n", "    ignore_cols = set([\"d\", \"d_int\", TARGET_COL])\n", "    # exclude calendar string columns\n", "    str_cols = [c for c in df.columns if df[c].dtype == object and c not in cat_cols]\n", "    num_cols = [c for c in df.columns if c not in cat_cols + bin_cols + list(ignore_cols) + str_cols]\n", "    # drop heavy 182-window features to reduce memory\n", "    num_cols = [c for c in num_cols if \"182\" not in c]\n", "    train_df = df[df[\"d_int\"] <= TRAIN_END].copy()\n", "    val_df = df[(df[\"d_int\"] > TRAIN_END) & (df[\"d_int\"] <= VAL_END)].copy()\n", "    # types\n", "    for col in cat_cols:\n", "        if col in train_df:\n", "            cats = pd.CategoricalDtype(categories=train_df[col].dropna().unique())\n", "            train_df[col] = train_df[col].astype(cats)\n", "            val_df[col] = val_df[col].astype(cats)\n", "    for col in bin_cols:\n", "        train_df[col] = train_df[col].astype(\"int8\")\n", "        val_df[col] = val_df[col].astype(\"int8\")\n", "    for col in num_cols:\n", "        train_df[col] = train_df[col].astype(\"float32\")\n", "        val_df[col] = val_df[col].astype(\"float32\")\n", "    feature_cols = num_cols + bin_cols + cat_cols\n", "    return train_df, val_df, feature_cols, cat_cols"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_eval(train_df: pd.DataFrame, val_df: pd.DataFrame, feature_cols: List[str], cat_cols: List[str], params: dict):\n", "    X_train = train_df[feature_cols]\n", "    y_train = train_df[TARGET_COL].astype(\"float32\")\n", "    X_val = val_df[feature_cols]\n", "    y_val = val_df[TARGET_COL].astype(\"float32\")\n", "    model = lgb.LGBMRegressor(**params)\n", "    model.fit(\n", "        X_train,\n", "        y_train,\n", "        eval_set=[(X_val, y_val)],\n", "        eval_metric=\"rmse\",\n", "        categorical_feature=[c for c in cat_cols if c in feature_cols],\n", "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n", "    )\n", "    preds = model.predict(X_val, num_iteration=model.best_iteration_)\n", "    rmse = float(np.sqrt(mean_squared_error(y_val, preds)))\n", "    mape = safe_mape(y_val.values, preds)\n", "    return {\"rmse\": rmse, \"mape\": mape, \"iter\": model.best_iteration_}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main() -> None:\n", "    summary = []\n", "    for store in STORES:\n", "        print(f\"\\n=== Training long-season model on {store} ===\")\n", "        train_df, val_df, feature_cols, cat_cols = build_datasets(store)\n", "        metrics = train_eval(train_df, val_df, feature_cols, cat_cols, PARAMS)\n", "        print(f\"{store}: RMSE {metrics['rmse']:.4f} | MAPE {metrics['mape']:.4f} | iter {metrics['iter']}\")\n", "        summary.append({\"store\": store, **metrics})\n", "    with SUMMARY_PATH.open(\"w\", encoding=\"utf-8\") as f:\n", "        json.dump(summary, f, ensure_ascii=False, indent=2)\n", "    print(f\"Saved summary to {SUMMARY_PATH}\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}