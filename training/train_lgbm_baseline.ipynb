{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#!/usr/bin/env python3\n", "# -*- coding: utf-8 -*-\n", "\"\"\"\n", "LightGBM baseline training script for M5-style data with WRMSSE monitoring."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["Usage:\n", "    python train_lgbm_baseline.py"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["The workflow:\n", "1. Reads per-store processed CSVs with rolling/lags.\n", "2. Builds core (low-variance) versus extra feature sets.\n", "3. Trains LightGBM with RMSE early stopping but WRMSSE callback every WRMSSE_EVAL_FREQ.\n", "4. Optionally blends with tweaked models if WRMSSE improves.\n", "5. Logs metrics/wrmsse history to weight_v2/summary_delay120_v2.json and blend weights.\n", "\"\"\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import annotations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gc\n", "import json\n", "import subprocess\n", "import time\n", "from pathlib import Path\n", "import argparse\n", "from typing import Callable, List"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import lightgbm as lgb\n", "import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import mean_squared_error"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from wrmsse_official import WRMSSEEvaluator"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATA_DIR = Path(\"newfinaldata\")  # switch to processed_v2/v3 as needed\n", "TRAIN_END = 1913\n", "VAL_END = 1941"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WRMSSE_SALES_FILE = Path(\"data/sales_train_validation.csv\")\n", "WRMSSE_ENABLED = True\n", "WRMSSE_EVAL_FREQ = 40\n", "WRMSSE_PATIENCE = 4\n", "WRMSSE_IMPROVE_TOL = 1e-5\n", "try:\n", "    _WRMSSE_OFFICIAL = WRMSSEEvaluator(sales_file=WRMSSE_SALES_FILE)\n", "except Exception as e:\n", "    print(f\"Warning: WRMSSEEvaluator import/init failed ({e}); wrmsse will raise if called.\")\n", "    _WRMSSE_OFFICIAL = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if _WRMSSE_OFFICIAL is None:\n", "    WRMSSE_ENABLED = False"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["WEIGHT_DIR = Path(\"weight_v2\")\n", "WEIGHT_DIR.mkdir(parents=True, exist_ok=True)\n", "SUMMARY_PATH = WEIGHT_DIR / \"summary_delay120_v2.json\"\n", "BLEND_WEIGHT_PATH = WEIGHT_DIR / \"delay_120_weight_v2.json\"\n", "VISUALIZE_SCRIPT = Path(\"visualize_and_blend.py\")\n", "VISUALIZE_BASE = Path(\"future_finaldata/submission_with_val.csv\")\n", "VISUALIZE_ALT = Path(\"future_finaldata/submission_with_val_cmodel.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STATE_GROUPS = {\n", "    \"CA\": [\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"],\n", "    \"TX\": [\"TX_1\", \"TX_2\", \"TX_3\"],\n", "    \"WI\": [\"WI_1\", \"WI_2\", \"WI_3\"],\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STATE_PARAM_SOURCE = {\n", "    \"CA\": \"CA_1\",\n", "    \"WI\": \"WI_1\",\n", "    \"TX\": \"TX_1\",\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STORE_LIST: List[str] = [\n", "    \"CA_1\",\n", "    \"CA_2\",\n", "    \"CA_3\",\n", "    \"CA_4\",\n", "    \"WI_1\",\n", "    \"WI_2\",\n", "    \"WI_3\",\n", "    \"TX_1\",\n", "    \"TX_2\",\n", "    \"TX_3\",\n", "]\n", "SEARCH_STORES = {\"CA_1\", \"TX_1\"}\n", "STORE_PARAMS = {\n", "    \"CA_1\": {\"learning_rate\": 0.035, \"num_leaves\": 220, \"min_data_in_leaf\": 180},\n", "    \"CA_2\": {\"learning_rate\": 0.035, \"num_leaves\": 220, \"min_data_in_leaf\": 180},\n", "    \"CA_3\": {\"learning_rate\": 0.035, \"num_leaves\": 220, \"min_data_in_leaf\": 180},\n", "    \"CA_4\": {\"learning_rate\": 0.035, \"num_leaves\": 220, \"min_data_in_leaf\": 180},\n", "    \"TX_1\": {\"learning_rate\": 0.035, \"num_leaves\": 200, \"min_data_in_leaf\": 200},\n", "    \"TX_2\": {\"learning_rate\": 0.035, \"num_leaves\": 200, \"min_data_in_leaf\": 200},\n", "    \"TX_3\": {\"learning_rate\": 0.035, \"num_leaves\": 200, \"min_data_in_leaf\": 200},\n", "    \"WI_1\": {\"learning_rate\": 0.036, \"num_leaves\": 200, \"min_data_in_leaf\": 220},\n", "    \"WI_2\": {\"learning_rate\": 0.036, \"num_leaves\": 200, \"min_data_in_leaf\": 220},\n", "    \"WI_3\": {\"learning_rate\": 0.036, \"num_leaves\": 200, \"min_data_in_leaf\": 220},\n", "}\n", "DEPT_FILTER: List[str] = []\n", "ITEM_FILTER: List[str] = []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CORE_LAG_SCALED = [\"lag_1_scaled\", \"lag_7_scaled\", \"lag_28_scaled\"]\n", "CORE_ROLL_SCALED = [\"rolling_mean_7_scaled\", \"rolling_mean_14_scaled\", \"rolling_std_7_scaled\"]\n", "CORE_PRICE_SCALED = [\n", "    \"sell_price_scaled\",\n", "    \"baseline_price_scaled\",\n", "    \"discount_scaled\",\n", "    \"promo_intensity_scaled\",\n", "    \"price_ratio_scaled\",\n", "    \"discount_pct_scaled\",\n", "]\n", "CORE_PROMO_SCALED = [\n", "    \"snap_wday_scaled\",\n", "    \"promo_holiday_scaled\",\n", "    \"promo_wday_sin_scaled\",\n", "    \"promo_wday_cos_scaled\",\n", "    \"discount_snap_scaled\",\n", "]\n", "CORE_GENERATED_SCALED = [\n", "    \"item_freq_scaled\",\n", "    \"id_freq_scaled\",\n", "    \"item_te_scaled\",\n", "    \"id_te_scaled\",\n", "]\n", "CORE_NUM_SCALED = CORE_LAG_SCALED + CORE_ROLL_SCALED + CORE_PRICE_SCALED + CORE_PROMO_SCALED"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["EXTRA_LAG_SCALED = [\"lag_14_scaled\", \"lag_56_scaled\", \"lag_84_scaled\"]\n", "EXTRA_ROLL_MEAN = [\n", "    \"rolling_mean_7_scaled\",\n", "    \"rolling_mean_14_scaled\",\n", "    \"rolling_mean_28_scaled\",\n", "    \"rolling_mean_30_scaled\",\n", "    \"rolling_mean_56_scaled\",\n", "    \"rolling_mean_84_scaled\",\n", "]\n", "EXTRA_ROLL_STD = [\n", "    \"rolling_std_7_scaled\",\n", "    \"rolling_std_14_scaled\",\n", "    \"rolling_std_28_scaled\",\n", "    \"rolling_std_30_scaled\",\n", "    \"rolling_std_56_scaled\",\n", "    \"rolling_std_84_scaled\",\n", "]\n", "EXTRA_ROLL_MED = [\n", "    \"rolling_median_7_scaled\",\n", "    \"rolling_median_14_scaled\",\n", "    \"rolling_median_28_scaled\",\n", "    \"rolling_median_30_scaled\",\n", "    \"rolling_median_56_scaled\",\n", "    \"rolling_median_84_scaled\",\n", "]\n", "EXTRA_ROLL_MIN = [\n", "    \"rolling_min_7_scaled\",\n", "    \"rolling_min_14_scaled\",\n", "    \"rolling_min_28_scaled\",\n", "    \"rolling_min_30_scaled\",\n", "    \"rolling_min_56_scaled\",\n", "    \"rolling_min_84_scaled\",\n", "]\n", "EXTRA_ROLL_MAX = [\n", "    \"rolling_max_7_scaled\",\n", "    \"rolling_max_14_scaled\",\n", "    \"rolling_max_28_scaled\",\n", "    \"rolling_max_30_scaled\",\n", "    \"rolling_max_56_scaled\",\n", "    \"rolling_max_84_scaled\",\n", "]\n", "EXTRA_PRICE_MOMENTUM = [\n", "    \"sell_price_week_chg_scaled\",\n", "    \"sell_price_month_chg_scaled\",\n", "    \"sell_price_z28_scaled\",\n", "    \"discount_week_chg_scaled\",\n", "    \"discount_month_chg_scaled\",\n", "    \"discount_z28_scaled\",\n", "]\n", "EXTRA_SEASONAL = [\n", "    \"promo_streak_scaled\",\n", "    \"holiday_streak_scaled\",\n", "    \"days_since_holiday_scaled\",\n", "    \"days_until_holiday_scaled\",\n", "]\n", "SELECTED_EXTRAS = [\n", "    \"lag_14_scaled\",\n", "    \"lag_56_scaled\",\n", "    \"lag_84_scaled\",\n", "    \"rolling_mean_7_scaled\",\n", "    \"rolling_mean_14_scaled\",\n", "    \"rolling_mean_28_scaled\",\n", "    \"sell_price_week_chg_scaled\",\n", "    \"sell_price_month_chg_scaled\",\n", "    \"discount_week_chg_scaled\",\n", "    \"promo_streak_scaled\",\n", "    \"days_since_holiday_scaled\",\n", "]\n", "EXTRA_NUM_SCALED = (\n", "    EXTRA_LAG_SCALED\n", "    + EXTRA_ROLL_MEAN\n", "    + EXTRA_ROLL_STD\n", "    + EXTRA_ROLL_MED\n", "    + EXTRA_ROLL_MIN\n", "    + EXTRA_ROLL_MAX\n", "    + EXTRA_PRICE_MOMENTUM\n", "    + EXTRA_SEASONAL\n", ")\n", "NUM_SCALED = CORE_NUM_SCALED + EXTRA_NUM_SCALED"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CYCLIC = [\n", "    \"wday_sin\",\n", "    \"wday_cos\",\n", "    \"month_sin\",\n", "    \"month_cos\",\n", "    \"quarter_sin\",\n", "    \"quarter_cos\",\n", "]\n", "BIN_COLS = [\"snap_CA\", \"snap_TX\", \"snap_WI\", \"IsHoliday\", \"IsPromotion\"]\n", "CAT_COLS = [\n", "    \"state_id\",\n", "    \"store_id\",\n", "    \"cat_id\",\n", "    \"dept_id\",\n", "    \"event_name_1\",\n", "    \"event_type_1\",\n", "    \"event_name_2\",\n", "    \"event_type_2\",\n", "    \"item_id\",\n", "    \"id\",\n", "]\n", "TARGET_COL = \"sales\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["BASE_PARAMS = dict(\n", "    objective=\"regression\",\n", "    metric=[\"rmse\", \"mape\"],\n", "    learning_rate=0.04,\n", "    num_leaves=255,\n", "    max_depth=10,\n", "    feature_fraction=0.85,\n", "    bagging_fraction=0.85,\n", "    bagging_freq=5,\n", "    min_data_in_leaf=240,\n", "    lambda_l1=0.5,\n", "    lambda_l2=1.0,\n", "    n_estimators=2400,\n", "    max_bin=511,\n", ")\n", "DO_RANDOM_SEARCH = False\n", "N_TRIALS = 10\n", "EARLY_STOPPING = 50\n", "ENABLE_BLEND = False\n", "BLEND_WEIGHTS = [0.0, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n", "NO_BLEND_STORES = set()\n", "USE_CV = False\n", "CV_STORES = {\"CA_3\", \"WI_2\"}\n", "CV_VAL_LEN = 28\n", "CV_FOLDS = 2\n", "USE_LOG1P_TARGET = False\n", "TIME_DECAY = True\n", "DECAY_HALF_LIFE = 180\n", "GROUP_DEFINITIONS = {\n", "    \"group_a\": [\"CA_1\", \"CA_2\", \"CA_4\", \"TX_1\"],\n", "    \"group_b\": [\"CA_3\", \"WI_1\", \"WI_2\", \"TX_3\"],\n", "}\n", "CANDIDATE_DIR = Path(\"weight_v2\")\n", "CANDIDATE_PATTERN = CANDIDATE_DIR / \"group_params_{group}_{model_type}.json\"\n", "GROUP_CANDIDATE_CACHE: dict[str, dict[str, list[dict]]] = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["CHUNK_FILE_PATH: Path | None = None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["STORE_TO_GROUP = {store: grp for grp, stores in GROUP_DEFINITIONS.items() for store in stores}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def prune_low_variance(\n", "    train_df: pd.DataFrame, cols: list[str], threshold: float = 1e-6\n", ") -> list[str]:\n", "    present = [c for c in cols if c in train_df.columns]\n", "    if not present:\n", "        return []\n", "    chunk_size = 1_000_000\n", "    sum_x = {c: 0.0 for c in present}\n", "    sum_x2 = {c: 0.0 for c in present}\n", "    count = 0\n", "    for chunk in np.array_split(train_df[present], max(1, len(train_df) // chunk_size)):\n", "        chunk = chunk.astype(\"float64\", copy=False)\n", "        cnt = len(chunk)\n", "        count += cnt\n", "        for c in present:\n", "            series = chunk[c]\n", "            sum_x[c] += series.sum()\n", "            sum_x2[c] += (series ** 2).sum()\n", "    kept = []\n", "    if count == 0:\n", "        return present\n", "    for c in present:\n", "        mean = sum_x[c] / count\n", "        var = sum_x2[c] / count - mean * mean\n", "        if var > threshold:\n", "            kept.append(c)\n", "    return kept"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def _resolve_store_override(store: str) -> dict | None:\n", "    override = STORE_PARAMS.get(store)\n", "    if override:\n", "        return override\n", "    for state, stores in STATE_GROUPS.items():\n", "        if store in stores:\n", "            source = STATE_PARAM_SOURCE.get(state)\n", "            if source and source in STORE_PARAMS:\n", "                return STORE_PARAMS[source]\n", "    return None"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def merge_store_params(store: str, override: dict | None = None) -> dict:\n", "    params = BASE_PARAMS.copy()\n", "    store_override = _resolve_store_override(store)\n", "    if store_override:\n", "        params.update(store_override)\n", "    if override:\n", "        params.update(override)\n", "    return params"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_group_candidates(group: str, model_type: str) -> list[dict]:\n", "    group_cache = GROUP_CANDIDATE_CACHE.setdefault(group, {})\n", "    if model_type in group_cache:\n", "        return group_cache[model_type]\n", "    pattern = str(CANDIDATE_PATTERN)\n", "    path = Path(pattern.format(group=group, model_type=model_type))\n", "    params: list[dict] = []\n", "    if path.exists():\n", "        try:\n", "            with path.open(\"r\", encoding=\"utf-8\") as f:\n", "                params = [entry.get(\"params\", {}) for entry in json.load(f) if isinstance(entry, dict)]\n", "        except Exception:\n", "            params = []\n", "    group_cache[model_type] = params\n", "    return params"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_store(store: str, data_dir: Path, usecols: List[str]) -> pd.DataFrame:\n", "    if CHUNK_FILE_PATH:\n", "        path = CHUNK_FILE_PATH\n", "    else:\n", "        path = data_dir / f\"processed_{store}.csv\"\n", "    if not path.exists():\n", "        raise FileNotFoundError(path)\n", "    chunks: list[pd.DataFrame] = []\n", "    for chunk in pd.read_csv(path, usecols=usecols, chunksize=1_000_000):\n", "        chunk[\"d_int\"] = chunk[\"d\"].str.replace(\"d_\", \"\", regex=False).astype(int)\n", "        chunks.append(chunk)\n", "    df = pd.concat(chunks, ignore_index=True)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def set_categorical(train: pd.Series, val: pd.Series) -> tuple[pd.Series, pd.Series]:\n", "    cats = pd.CategoricalDtype(categories=train.dropna().unique())\n", "    return train.astype(cats), val.astype(cats)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def build_datasets(\n", "    stores: List[str], dept_filter: List[str] | None = None, item_filter: List[str] | None = None\n", ") -> tuple[pd.DataFrame, pd.DataFrame, List[str], List[str]]:\n", "    feature_cols_available = NUM_SCALED + CYCLIC + BIN_COLS + CAT_COLS\n", "    generated_cols = [\n", "        \"item_freq\",\n", "        \"id_freq\",\n", "        \"item_freq_scaled\",\n", "        \"id_freq_scaled\",\n", "        \"item_te\",\n", "        \"id_te\",\n", "        \"item_te_scaled\",\n", "        \"id_te_scaled\",\n", "    ]\n", "    usecols = [c for c in feature_cols_available + [TARGET_COL, \"d\"] if c not in generated_cols]\n", "    dfs = [read_store(store, DATA_DIR, usecols) for store in stores]\n", "    df = pd.concat(dfs, ignore_index=True)\n", "    del dfs\n", "    gc.collect()\n", "    df = df.loc[:, ~df.columns.duplicated()]\n", "    if dept_filter:\n", "        df = df[df[\"dept_id\"].isin(dept_filter)]\n", "    if item_filter:\n", "        df = df[df[\"item_id\"].isin(item_filter)]\n", "    df[\"id\"] = df[\"id\"].str.replace(\"_evaluation\", \"_validation\", regex=False)\n", "    numeric_cols = [\n", "        c\n", "        for c in feature_cols_available\n", "        if c in df.columns and c not in CAT_COLS\n", "    ]\n", "    for col in numeric_cols:\n", "        df.loc[:, col] = df[col].astype(\"float32\")\n", "    train_df = df[df[\"d_int\"] <= TRAIN_END]\n", "    val_df = df[(df[\"d_int\"] > TRAIN_END) & (df[\"d_int\"] <= VAL_END)]\n", "    if val_df.empty:\n", "        raise ValueError(\"Validation set empty.\")\n", "    for col in CAT_COLS:\n", "        train_df[col], val_df[col] = set_categorical(train_df[col].astype(str), val_df[col].astype(str))\n", "    item_counts = train_df[\"item_id\"].value_counts()\n", "    id_counts = train_df[\"id\"].value_counts()\n", "    train_df.loc[:, \"item_freq\"] = train_df[\"item_id\"].map(item_counts).fillna(0).astype(\"float32\")\n", "    val_df.loc[:, \"item_freq\"] = val_df[\"item_id\"].map(item_counts).fillna(0).astype(\"float32\")\n", "    train_df.loc[:, \"id_freq\"] = train_df[\"id\"].map(id_counts).fillna(0).astype(\"float32\")\n", "    val_df.loc[:, \"id_freq\"] = val_df[\"id\"].map(id_counts).fillna(0).astype(\"float32\")\n", "    for col in [\"item_freq\", \"id_freq\"]:\n", "        mn = train_df[col].min()\n", "        mx = train_df[col].max()\n", "        rng = mx - mn\n", "        if rng == 0:\n", "            train_df.loc[:, col + \"_scaled\"] = 0.0\n", "            val_df.loc[:, col + \"_scaled\"] = 0.0\n", "        else:\n", "            train_df.loc[:, col + \"_scaled\"] = (train_df[col] - mn) / rng\n", "            val_df.loc[:, col + \"_scaled\"] = (val_df[col] - mn) / rng\n", "    global_mean = train_df[TARGET_COL].mean()\n", "    def mean_encode(series: pd.Series, target: pd.Series, alpha: float = 5.0) -> pd.Series:\n", "        counts = series.value_counts()\n", "        means = train_df.groupby(series.name)[TARGET_COL].mean()\n", "        smoothing = (means * counts + alpha * global_mean) / (counts + alpha)\n", "        return series.map(smoothing)\n", "    train_df[\"item_te\"] = mean_encode(train_df[\"item_id\"], train_df[TARGET_COL])\n", "    val_df[\"item_te\"] = val_df[\"item_id\"].map(train_df.groupby(\"item_id\")[TARGET_COL].mean()).fillna(global_mean)\n", "    train_df[\"id_te\"] = mean_encode(train_df[\"id\"], train_df[TARGET_COL])\n", "    val_df[\"id_te\"] = val_df[\"id\"].map(train_df.groupby(\"id\")[TARGET_COL].mean()).fillna(global_mean)\n", "    for col in [\"item_te\", \"id_te\"]:\n", "        mn = train_df[col].min()\n", "        mx = train_df[col].max()\n", "        rng = mx - mn\n", "        if rng == 0:\n", "            train_df.loc[:, col + \"_scaled\"] = 0.0\n", "            val_df.loc[:, col + \"_scaled\"] = 0.0\n", "        else:\n", "            train_df.loc[:, col + \"_scaled\"] = (train_df[col] - mn) / rng\n", "            val_df.loc[:, col + \"_scaled\"] = (val_df[col] - mn) / rng\n", "    numeric_core_candidates = CORE_NUM_SCALED + CORE_GENERATED_SCALED\n", "    kept_core_numeric = prune_low_variance(train_df, numeric_core_candidates)\n", "    feature_cols_core = list(dict.fromkeys(kept_core_numeric + CORE_GENERATED_SCALED + CYCLIC + BIN_COLS + CAT_COLS))\n", "    extra_candidates = [c for c in EXTRA_NUM_SCALED if c not in kept_core_numeric]\n", "    # limit extras to high variance subset\n", "    extra_candidates = [c for c in extra_candidates if c in SELECTED_EXTRAS]\n", "    kept_extra = prune_low_variance(train_df, extra_candidates)\n", "    feature_cols_full = list(dict.fromkeys(feature_cols_core + kept_extra))\n", "    for col in BIN_COLS:\n", "        train_df[col] = train_df[col].astype(\"int8\")\n", "        val_df[col] = val_df[col].astype(\"int8\")\n", "    for col in NUM_SCALED + CYCLIC:\n", "        train_df[col] = train_df[col].astype(\"float32\")\n", "        val_df[col] = val_df[col].astype(\"float32\")\n", "    return train_df, val_df, feature_cols_core, feature_cols_full"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def safe_mape(y_true: np.ndarray, y_pred: np.ndarray, min_denom: float = 1e-3) -> float:\n", "    mask = np.abs(y_true) > min_denom\n", "    if not mask.any():\n", "        return float(\"nan\")\n", "    return float(np.mean(np.abs(y_true[mask] - y_pred[mask]) / np.abs(y_true[mask])))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-6) -> float:\n", "    num = np.abs(y_pred - y_true)\n", "    denom = (np.abs(y_true) + np.abs(y_pred)).clip(min=eps)\n", "    return float(np.mean(2.0 * num / denom))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def wrmsse(val_df: pd.DataFrame, preds: np.ndarray) -> float:\n", "    if not WRMSSE_ENABLED or _WRMSSE_OFFICIAL is None:\n", "        return 0.0\n", "    valid_ids = val_df[\"id\"].astype(str).str.strip()\n", "    available_meta = _WRMSSE_OFFICIAL.meta[\"id\"].astype(str).str.strip()\n", "    in_meta = valid_ids.isin(available_meta)\n", "    print(\n", "        f\"WRMSSE debug: {valid_ids.nunique()} unique ids, \"\n", "        f\"{in_meta.sum()} match metadata, {len(valid_ids) - in_meta.sum()} missing\"\n", "    )\n", "    # print(\"val sample ids:\", valid_ids.unique()[:10])\n", "    # print(\"first 10 meta ids:\", available_meta.unique()[:10])\n", "    # unmatched = valid_ids[~in_meta]\n", "    # print(\"sample unmatched ids:\", unmatched.unique()[:10])\n", "    y_true = val_df[[\"id\", \"d\", TARGET_COL]].copy()\n", "    y_true[\"d\"] = y_true[\"d\"].astype(str).str.replace(\"d_\", \"\").astype(int)\n", "    y_true = y_true.rename(columns={TARGET_COL: \"sales\"})\n", "    y_pred = y_true.copy()\n", "    y_pred[\"sales\"] = preds.astype(\"float32\")\n", "    score, _ = _WRMSSE_OFFICIAL.compute_wrmsse(y_true, y_pred)\n", "    return float(score)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def make_wrmsse_monitor(\n", "    val_df: pd.DataFrame, feature_cols: List[str], eval_freq: int, patience: int\n", ") -> tuple[Callable, dict]:\n", "    state = {\"history\": [], \"best_score\": float(\"inf\"), \"best_iter\": 0, \"wait\": 0}\n", "    def callback(env):\n", "        if not WRMSSE_ENABLED or _WRMSSE_OFFICIAL is None:\n", "            return\n", "        if eval_freq <= 0 or env.iteration % eval_freq != 0:\n", "            return\n", "        preds = env.model.predict(val_df[feature_cols], num_iteration=env.iteration)\n", "        score = wrmsse(val_df, preds)\n", "        state[\"history\"].append((env.iteration, score))\n", "        if score + WRMSSE_IMPROVE_TOL < state[\"best_score\"]:\n", "            state[\"best_score\"] = score\n", "            state[\"best_iter\"] = env.iteration\n", "            state[\"wait\"] = 0\n", "        else:\n", "            state[\"wait\"] += 1\n", "            if state[\"wait\"] >= patience:\n", "                env.model.stop_training = True\n", "    return callback, state"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_lgbm(train_df: pd.DataFrame, val_df: pd.DataFrame, feature_cols: List[str], params: dict) -> tuple[lgb.LGBMRegressor, dict]:\n", "    X_train = train_df[feature_cols]\n", "    y_train_raw = train_df[TARGET_COL].astype(\"float32\")\n", "    X_val = val_df[feature_cols]\n", "    y_val_raw = val_df[TARGET_COL].astype(\"float32\")\n", "    sample_weight = None\n", "    if TIME_DECAY:\n", "        age = TRAIN_END - train_df[\"d_int\"]\n", "        sample_weight = np.exp(-np.log(2) * age / DECAY_HALF_LIFE).astype(\"float32\")\n", "    if USE_LOG1P_TARGET:\n", "        y_train = np.log1p(y_train_raw)\n", "        y_val = np.log1p(y_val_raw)\n", "    else:\n", "        y_train = y_train_raw\n", "        y_val = y_val_raw\n", "    callbacks = [lgb.early_stopping(stopping_rounds=EARLY_STOPPING, verbose=50)]\n", "    wr_state = {\"history\": [], \"best_score\": float(\"inf\"), \"best_iter\": 0, \"wait\": 0}\n", "    if WRMSSE_ENABLED and _WRMSSE_OFFICIAL is not None:\n", "        wr_cb, wr_state = make_wrmsse_monitor(val_df, feature_cols, WRMSSE_EVAL_FREQ, WRMSSE_PATIENCE)\n", "        callbacks.append(wr_cb)\n", "    model = lgb.LGBMRegressor(**params)\n", "    start = time.perf_counter()\n", "    model.fit(\n", "        X_train,\n", "        y_train,\n", "        sample_weight=sample_weight,\n", "        eval_set=[(X_val, y_val)],\n", "        eval_metric=\"rmse\",\n", "        categorical_feature=CAT_COLS,\n", "        callbacks=callbacks,\n", "    )\n", "    train_time = time.perf_counter() - start\n", "    preds = model.predict(X_val, num_iteration=model.best_iteration_)\n", "    if USE_LOG1P_TARGET:\n", "        preds = np.expm1(preds)\n", "        y_val_used = y_val_raw.values\n", "    else:\n", "        y_val_used = y_val.values\n", "    rmse = float(np.sqrt(mean_squared_error(y_val_used, preds)))\n", "    mape = safe_mape(y_val_used, preds)\n", "    smape_val = smape(y_val_used, preds)\n", "    wrmsse_val = wrmsse(val_df, preds)\n", "    if not wr_state[\"history\"] or wr_state[\"history\"][-1][0] != model.best_iteration_:\n", "        wr_state[\"history\"].append((model.best_iteration_, wrmsse_val))\n", "    best_wrmsse = (\n", "        min(wr_state[\"best_score\"], wrmsse_val)\n", "        if wr_state[\"best_score\"] != float(\"inf\")\n", "        else wrmsse_val\n", "    )\n", "    best_wrmsse_iter = wr_state[\"best_iter\"] or model.best_iteration_\n", "    metrics = {\n", "        \"rmse\": rmse,\n", "        \"mape\": mape,\n", "        \"smape\": smape_val,\n", "        \"wrmsse\": wrmsse_val,\n", "        \"iter\": model.best_iteration_,\n", "        \"train_time\": train_time,\n", "        \"wrmsse_history\": wr_state[\"history\"],\n", "        \"best_wrmsse\": float(best_wrmsse),\n", "        \"best_wrmsse_iter\": best_wrmsse_iter,\n", "    }\n", "    print(\n", "        f\"Validation RMSE: {rmse:.4f} | MAPE (>0.001 mask): {mape:.4f} | SMAPE: {smape_val:.4f} | WRMSSE: {wrmsse_val:.4f}\"\n", "    )\n", "    return model, metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_candidate_for_store(\n", "    train_df: pd.DataFrame, val_df: pd.DataFrame, feature_cols: List[str], params: dict\n", ") -> dict | None:\n", "    if val_df.empty:\n", "        return None\n", "    model, metrics = train_lgbm(train_df, val_df, feature_cols, params)\n", "    prev_mask = train_df[\"d_int\"].between(1886, TRAIN_END)\n", "    prev_df = train_df[prev_mask]\n", "    preds_val = model.predict(val_df[feature_cols], num_iteration=model.best_iteration_)\n", "    score_val = wrmsse(val_df, preds_val)\n", "    preds_prev = None\n", "    score_prev = float(\"inf\")\n", "    if not prev_df.empty:\n", "        preds_prev = model.predict(prev_df[feature_cols])\n", "        score_prev = wrmsse(prev_df, preds_prev)\n", "    avg_score = (score_prev + score_val) / 2.0\n", "    return {\n", "        \"model\": model,\n", "        \"params\": params,\n", "        \"metrics\": metrics,\n", "        \"score_prev\": score_prev,\n", "        \"score_val\": score_val,\n", "        \"score_avg\": avg_score,\n", "        \"preds_val\": preds_val,\n", "        \"val_df\": val_df,\n", "    }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate_candidates_for_store(\n", "    train_df: pd.DataFrame,\n", "    val_df: pd.DataFrame,\n", "    feature_cols: List[str],\n", "    candidate_params: list[dict],\n", ") -> dict | None:\n", "    best = None\n", "    if not candidate_params:\n", "        return None\n", "    for params in candidate_params:\n", "        result = evaluate_candidate_for_store(train_df, val_df, feature_cols, params)\n", "        if not result:\n", "            continue\n", "        if best is None or result[\"score_avg\"] < best[\"score_avg\"]:\n", "            best = result\n", "    return best"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def find_best_blend(val_df: pd.DataFrame, preds_base: np.ndarray, preds_alt: np.ndarray) -> dict:\n", "    y_val = val_df[TARGET_COL].astype(\"float32\").values\n", "    best = {\n", "        \"type\": \"main\",\n", "        \"w_base\": 1.0,\n", "        \"w_alt\": 0.0,\n", "        \"wrmsse\": wrmsse(val_df, preds_base),\n", "    }\n", "    if preds_alt is None:\n", "        return best\n", "    for w in BLEND_WEIGHTS:\n", "        blended = w * preds_base + (1.0 - w) * preds_alt\n", "        score = wrmsse(val_df, blended)\n", "        if score < best[\"wrmsse\"]:\n", "            best = {\n", "                \"type\": \"2model\",\n", "                \"w_base\": w,\n", "                \"w_alt\": 1.0 - w,\n", "                \"wrmsse\": score,\n", "            }\n", "    return best"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def finalize_blend_weights(decisions: list[dict]) -> dict:\n", "    weights = {}\n", "    for dec in decisions:\n", "        store = dec[\"stores\"][0]\n", "        auto_step = dec[\"auto_decision\"]\n", "        if auto_step == \"allow\" and dec[\"main_result\"] and dec[\"c_result\"]:\n", "            blend_info = find_best_blend(\n", "                dec[\"main_result\"][\"val_df\"],\n", "                dec[\"main_result\"][\"preds_val\"],\n", "                dec[\"c_result\"][\"preds_val\"],\n", "            )\n", "        else:\n", "            # default to main model only\n", "            base_preds = (\n", "                dec[\"main_result\"][\"preds_val\"] if dec[\"main_result\"] else None\n", "            )\n", "            blend_info = {\n", "                \"type\": \"main\",\n", "                \"w_base\": 1.0,\n", "                \"w_alt\": 0.0,\n", "                \"wrmsse\": wrmsse(dec[\"main_result\"][\"val_df\"], base_preds)\n", "                if base_preds is not None\n", "                else None,\n", "            }\n", "        weights[store] = blend_info\n", "    return weights"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_chunk_key() -> str | None:\n", "    if CHUNK_FILE_PATH is None:\n", "        return None\n", "    return CHUNK_FILE_PATH.name"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def merge_weight_entries(\n", "    base_weights: dict[str, dict], new_weights: dict[str, dict], chunk_key: str | None\n", ") -> dict[str, dict]:\n", "    merged = base_weights.copy()\n", "    for store, entry in new_weights.items():\n", "        updated = dict(entry)\n", "        if chunk_key:\n", "            prev = merged.get(store, {})\n", "            history = prev.get(\"chunk_history\", [])\n", "            history = list(history) if isinstance(history, list) else []\n", "            snapshot = updated.copy()\n", "            snapshot.pop(\"chunk_history\", None)\n", "            history.append({\"chunk\": chunk_key, \"weight\": snapshot})\n", "            updated[\"chunk_history\"] = history\n", "        merged[store] = updated\n", "    return merged\n", "def time_series_folds(df: pd.DataFrame) -> list[tuple[pd.DataFrame, pd.DataFrame]]:\n", "    folds = []\n", "    val1_start = TRAIN_END - CV_VAL_LEN + 1\n", "    val1_end = TRAIN_END\n", "    train1 = df[df[\"d_int\"] < val1_start]\n", "    val1 = df[(df[\"d_int\"] >= val1_start) & (df[\"d_int\"] <= val1_end)]\n", "    if not val1.empty and len(train1) > 0:\n", "        folds.append((train1, val1))\n", "    train2 = df[df[\"d_int\"] <= TRAIN_END]\n", "    val2 = df[(df[\"d_int\"] > TRAIN_END) & (df[\"d_int\"] <= VAL_END)]\n", "    if not val2.empty and len(train2) > 0:\n", "        folds.append((train2, val2))\n", "    return folds"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def cv_evaluate(df: pd.DataFrame, feature_cols: List[str], params: dict) -> dict:\n", "    folds = time_series_folds(df)\n", "    metrics_list = []\n", "    for i, (tr, va) in enumerate(folds, 1):\n", "        print(f\"  CV fold {i}: train={len(tr):,}, val={len(va):,}\")\n", "        _, metrics = train_lgbm(tr, va, feature_cols, params)\n", "        metrics_list.append(metrics)\n", "    avg = {k: float(np.nanmean([m[k] for m in metrics_list])) for k in [\"rmse\", \"mape\", \"smape\", \"wrmsse\"]}\n", "    avg[\"iter\"] = int(np.nanmean([m[\"iter\"] for m in metrics_list]))\n", "    return avg"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tweak_params_for_blend(params: dict) -> dict:\n", "    p = params.copy()\n", "    p[\"learning_rate\"] = max(1e-4, params.get(\"learning_rate\", 0.03) * 0.8)\n", "    p[\"n_estimators\"] = int(params.get(\"n_estimators\", 1800) * 1.3)\n", "    p[\"num_leaves\"] = int(params.get(\"num_leaves\", 255) * 1.2)\n", "    p[\"min_data_in_leaf\"] = max(50, int(params.get(\"min_data_in_leaf\", 200) * 0.8))\n", "    return p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tweak_params_for_blend2(params: dict) -> dict:\n", "    p = params.copy()\n", "    p[\"learning_rate\"] = min(0.1, params.get(\"learning_rate\", 0.03) * 1.1)\n", "    p[\"num_leaves\"] = max(31, int(params.get(\"num_leaves\", 255) * 0.7))\n", "    p[\"n_estimators\"] = int(params.get(\"n_estimators\", 1800) * 1.15)\n", "    p[\"feature_fraction\"] = min(1.0, params.get(\"feature_fraction\", 0.8) * 1.05)\n", "    p[\"bagging_fraction\"] = min(1.0, params.get(\"bagging_fraction\", 0.8) * 1.05)\n", "    p[\"min_data_in_leaf\"] = int(params.get(\"min_data_in_leaf\", 200) * 1.2)\n", "    return p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def tweak_params_for_blend3(params: dict) -> dict:\n", "    p = params.copy()\n", "    p[\"learning_rate\"] = max(1e-4, params.get(\"learning_rate\", 0.03) * 0.7)\n", "    p[\"num_leaves\"] = max(63, int(params.get(\"num_leaves\", 255) * 0.8))\n", "    p[\"min_data_in_leaf\"] = int(params.get(\"min_data_in_leaf\", 200) * 1.5)\n", "    p[\"n_estimators\"] = int(params.get(\"n_estimators\", 1800) * 1.2)\n", "    return p"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def parse_args() -> argparse.Namespace:\n", "    parser = argparse.ArgumentParser(description=\"Per-store training with chunked inputs.\")\n", "    parser.add_argument(\"--stores\", nargs=\"+\", help=\"Subset of stores to run (e.g. CA_1).\")\n", "    parser.add_argument(\n", "        \"--batch-size\",\n", "        type=int,\n", "        help=\"Number of stores to include in each batch when chunking the default store list.\",\n", "    )\n", "    parser.add_argument(\n", "        \"--batch-index\",\n", "        type=int,\n", "        default=0,\n", "        help=\"Zero-based index of the batch to run when --batch-size is set.\",\n", "    )\n", "    parser.add_argument(\n", "        \"--single-candidate\",\n", "        action=\"store_true\",\n", "        help=\"Evaluate only the first main/c candidate (for quick testing).\",\n", "    )\n", "    parser.add_argument(\n", "        \"--chunk-file\",\n", "        type=Path,\n", "        help=\"Path to the chunked CSV to read for this run.\",\n", "    )\n", "    return parser.parse_args()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def chunk_store_list(stores: List[str], batch_size: int) -> list[list[str]]:\n", "    return [stores[i : i + batch_size] for i in range(0, len(stores), batch_size)]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_existing_summary() -> list[dict]:\n", "    if SUMMARY_PATH.exists():\n", "        with SUMMARY_PATH.open(\"r\", encoding=\"utf-8\") as f:\n", "            return json.load(f)\n", "    return []"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_existing_weights() -> dict:\n", "    if BLEND_WEIGHT_PATH.exists():\n", "        with BLEND_WEIGHT_PATH.open(\"r\", encoding=\"utf-8\") as f:\n", "            data = json.load(f)\n", "        if isinstance(data, list):\n", "            combined = {}\n", "            for entry in data:\n", "                if isinstance(entry, dict):\n", "                    combined.update(entry)\n", "            return combined\n", "        if isinstance(data, dict):\n", "            return data\n", "    return {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main() -> None:\n", "    global CHUNK_FILE_PATH\n", "    args = parse_args()\n", "    CHUNK_FILE_PATH = args.chunk_file\n", "    existing_summary = load_existing_summary()\n", "    existing_stores = {entry[\"stores\"][0] for entry in existing_summary}\n", "    existing_weights = load_existing_weights()\n", "    tasks = []\n", "    store_sequence = args.stores or STORE_LIST\n", "    if args.batch_size:\n", "        batches = chunk_store_list(store_sequence, args.batch_size)\n", "        if args.batch_index >= len(batches):\n", "            raise ValueError(\"batch_index is out of range for the provided batch_size\")\n", "        store_sequence = batches[args.batch_index]\n", "    for store in store_sequence:\n", "        if store in existing_stores and args.chunk_file is None:\n", "            print(f\"Skipping {store} (already processed)\")\n", "            continue\n", "        override = None if store in SEARCH_STORES else STORE_PARAMS.get(store)\n", "        tasks.append((f\"store {store}\", [store], None, None, override))\n", "    if not tasks:\n", "        print(\"No new stores to process.\")\n", "        return\n", "    summary = []\n", "    decisions = []\n", "    for label, stores, dept_filter, item_filter, override_params in tasks:\n", "        print(f\"\\n===== Training {label} | stores: {stores} from {DATA_DIR} =====\")\n", "        train_df, val_df, feature_cols_core, feature_cols_full = build_datasets(\n", "            stores, dept_filter=dept_filter, item_filter=item_filter\n", "        )\n", "        feature_cols = list(dict.fromkeys(feature_cols_core))\n", "        extra_count = len(feature_cols_full) - len(feature_cols_core)\n", "        print(f\"Train rows: {len(train_df):,} | Val rows: {len(val_df):,}\")\n", "        print(\n", "            f\"Features: core={len(feature_cols_core):,}, extras={extra_count:,} (full={len(feature_cols_full):,})\"\n", "        )\n", "        group = STORE_TO_GROUP.get(stores[0], \"group_a\")\n", "        main_candidates = load_group_candidates(group, \"main\")\n", "        if not main_candidates:\n", "            main_candidates = [merge_store_params(stores[0], override_params)]\n", "        if args.single_candidate:\n", "            main_candidates = main_candidates[:1]\n", "        print(f\"Evaluating main/c candidate pools for group {group}\")\n", "        main_result = evaluate_candidates_for_store(train_df, val_df, feature_cols, main_candidates)\n", "        c_result = None\n", "        if ENABLE_BLEND:\n", "            c_candidates = load_group_candidates(group, \"c_model\")\n", "            if not c_candidates:\n", "                c_candidates = [merge_store_params(stores[0], override_params)]\n", "            if args.single_candidate:\n", "                c_candidates = c_candidates[:1]\n", "            c_result = evaluate_candidates_for_store(train_df, val_df, feature_cols_full, c_candidates)\n", "        auto_decision = \"ban\"\n", "        delta_wrmsse = None\n", "        if main_result and c_result:\n", "            delta_wrmsse = c_result[\"score_avg\"] - main_result[\"score_avg\"]\n", "            both_better = (\n", "                c_result[\"score_prev\"] < main_result[\"score_prev\"]\n", "                and c_result[\"score_val\"] < main_result[\"score_val\"]\n", "            )\n", "            if delta_wrmsse < -0.02 and both_better:\n", "                auto_decision = \"allow\"\n", "            elif abs(delta_wrmsse) <= 0.01:\n", "                auto_decision = \"neutral\"\n", "            else:\n", "                auto_decision = \"ban\"\n", "        if auto_decision == \"ban\":\n", "            NO_BLEND_STORES.add(stores[0])\n", "        decision = {\n", "            \"label\": label,\n", "            \"stores\": stores,\n", "            \"group\": group,\n", "            \"main_params\": main_result[\"params\"] if main_result else None,\n", "            \"main_wrmsse_prev\": main_result[\"score_prev\"] if main_result else None,\n", "            \"main_wrmsse_curr\": main_result[\"score_val\"] if main_result else None,\n", "            \"main_wrmsse\": main_result[\"score_avg\"] if main_result else None,\n", "            \"c_params\": c_result[\"params\"] if c_result else None,\n", "            \"c_wrmsse_prev\": c_result[\"score_prev\"] if c_result else None,\n", "            \"c_wrmsse_curr\": c_result[\"score_val\"] if c_result else None,\n", "            \"c_wrmsse\": c_result[\"score_avg\"] if c_result else None,\n", "            \"auto_decision\": auto_decision,\n", "            \"wrmsse_delta\": delta_wrmsse,\n", "            \"main_result\": main_result,\n", "            \"c_result\": c_result,\n", "        }\n", "        decisions.append(decision)\n", "        summary_entry = {k: decision[k] for k in decision if k not in {\"main_result\", \"c_result\"}}\n", "        if chunk_key := get_chunk_key():\n", "            summary_entry[\"chunk\"] = chunk_key\n", "        summary.append(summary_entry)\n", "    print(\"\\n===== Summary per task =====\")\n", "    for row in summary:\n", "        print(row)\n", "    try:\n", "        final_summary = existing_summary + summary\n", "        with SUMMARY_PATH.open(\"w\", encoding=\"utf-8\") as f:\n", "            json.dump(final_summary, f, ensure_ascii=False, indent=2)\n", "        blend_weights = finalize_blend_weights(decisions)\n", "        base_weights = existing_weights if isinstance(existing_weights, dict) else {}\n", "        final_weights = merge_weight_entries(base_weights, blend_weights, get_chunk_key())\n", "        with BLEND_WEIGHT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n", "            json.dump(final_weights, f, ensure_ascii=False, indent=2)\n", "        print(f\"\\nSaved summary to {SUMMARY_PATH} and blend weights to {BLEND_WEIGHT_PATH}\")\n", "        if (\n", "            VISUALIZE_SCRIPT.exists()\n", "            and VISUALIZE_BASE.exists()\n", "            and VISUALIZE_ALT.exists()\n", "        ):\n", "            cmd = [\n", "                \"python\",\n", "                str(VISUALIZE_SCRIPT),\n", "                \"--summary\",\n", "                str(SUMMARY_PATH),\n", "                \"--weights\",\n", "                str(BLEND_WEIGHT_PATH),\n", "                \"--base\",\n", "                str(VISUALIZE_BASE),\n", "                \"--alt\",\n", "                str(VISUALIZE_ALT),\n", "            ]\n", "            try:\n", "                subprocess.run(cmd, check=True)\n", "            except subprocess.SubprocessError as e:\n", "                print(f\"Visualization step failed: {e}\")\n", "    except Exception as e:\n", "        print(f\"Warning: failed to save summary/weights ({e})\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}