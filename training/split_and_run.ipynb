{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "from pathlib import Path\n", "import csv\n", "import subprocess"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["DATA_DIR = Path(\"newfinaldata\")\n", "CHUNK_DIR = Path(\"temp_chunk\")\n", "CHUNK_SIZE = 100_000"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_file(store: str) -> list[Path]:\n", "    path = DATA_DIR / f\"processed_{store}.csv\"\n", "    if not path.exists():\n", "        raise FileNotFoundError(path)\n", "    CHUNK_DIR.mkdir(parents=True, exist_ok=True)\n", "    parts = []\n", "    with path.open(\"r\", encoding=\"utf-8\") as src:\n", "        reader = csv.reader(src)\n", "        header = next(reader)\n", "        chunk_idx = 0\n", "        while True:\n", "            part = CHUNK_DIR / f\"{store}_part_{chunk_idx}.csv\"\n", "            rows_written = 0\n", "            with part.open(\"w\", encoding=\"utf-8\", newline=\"\") as out:\n", "                writer = csv.writer(out)\n", "                writer.writerow(header)\n", "                for _ in range(CHUNK_SIZE):\n", "                    try:\n", "                        writer.writerow(next(reader))\n", "                        rows_written += 1\n", "                    except StopIteration:\n", "                        break\n", "            if rows_written == 0:\n", "                part.unlink(missing_ok=True)\n", "                break\n", "            parts.append(part)\n", "            chunk_idx += 1\n", "            if rows_written < CHUNK_SIZE:\n", "                break\n", "    return parts"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def run_chunks(store: str):\n", "    parts = split_file(store)\n", "    for part in parts:\n", "        print(f\"Processing chunk {part}\")\n", "        subprocess.run(\n", "            [\n", "                \"python\",\n", "                \"train_lgbm_baseline.py\",\n", "                \"--stores\",\n", "                store,\n", "                \"--chunk-file\",\n", "                str(part),\n", "            ],\n", "            check=True,\n", "        )"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main():\n", "    parser = argparse.ArgumentParser()\n", "    parser.add_argument(\"--store\", required=True)\n", "    args = parser.parse_args()\n", "    run_chunks(args.store)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == \"__main__\":\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}